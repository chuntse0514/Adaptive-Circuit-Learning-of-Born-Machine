{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "\n",
    "dev = qml.device(\"default.qubit.torch\", wires=3)\n",
    "\n",
    "@qml.batch_params\n",
    "@qml.qnode(dev, interface='torch', diff_method='backprop')\n",
    "def circuit(x, weights):\n",
    "    qml.RX(x, wires=0)\n",
    "    qml.RY(0.2, wires=1)\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=[0, 1, 2])\n",
    "    return qml.expval(qml.Hadamard(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "x = torch.linspace(0.1, 0.5, batch_size)\n",
    "rng = np.random.default_rng(seed=1234)\n",
    "weights = rng.random((batch_size, 10, 3, 3), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10, 3, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 3, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.rand(10, 3, 3)\n",
    "\n",
    "batch_weights = torch.cat([weights.clone().unsqueeze(0) for _ in range(3)], dim=0)\n",
    "batch_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damon0514/pennylane-env/lib/python3.10/site-packages/pennylane/math/utils.py:225: UserWarning: Contains tensors of types {'autograd', 'torch'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0080, -0.0537, -0.1160],\n",
       "        [ 0.2579,  0.2735,  0.2831],\n",
       "        [-0.2395, -0.2436, -0.2440]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit(x, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pennylane-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
