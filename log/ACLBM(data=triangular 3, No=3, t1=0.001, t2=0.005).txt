('==== Found maximium gradient [0.6, 0.59999996, 0.3142858] of gate e^[Y0 Z1], '
 'e^[Y1 Z0], RY[0] ====')
learning rate =  0.10448448296053428
0.392396284757008 0.392396284757008 0.9689043164253235
0.24079586514914222 0.24079586514914222 0.5727344155311584
0.17422719202798387 0.17422719202798387 0.29963418841362
0.1684326671318182 0.1684326671318182 0.2571195065975189
0.18886158283365415 0.18886158283365415 0.4605296552181244
0.20164322885977826 0.20164322885977826 0.5976405143737793
0.19332636983839718 0.19332636983839718 0.5279319286346436
0.17965567239822464 0.17965567239822464 0.3996012210845947
0.17245165014523517 0.17245165014523517 0.32268473505973816
0.1706334759441253 0.1706334759441253 0.2906433939933777
0.16986523789165806 0.16986523789165806 0.2719580829143524
0.16792092569488232 0.16792092569488232 0.25411033630371094
0.16466442497233752 0.16466442497233752 0.23487462103366852
0.1608820438453796 0.1608820438453796 0.21431253850460052
0.15777377313159038 0.15777377313159038 0.1960902363061905
0.15658445617050343 0.15658445617050343 0.18781013786792755
0.1576985714948944 0.1576985714948944 0.19298437237739563
0.15997502249631085 0.15997502249631085 0.20387451350688934
0.16130918811047756 0.16130918811047756 0.20789341628551483
0.16022070310216113 0.16022070310216113 0.19710595905780792
0.15690907459436307 0.15690907459436307 0.17074313759803772
0.1528720662633042 0.1528720662633042 0.13372769951820374
0.14978692734626292 0.14978692734626292 0.09630751609802246
0.1486411095234675 0.1486411095234675 0.07731209695339203
0.14946618929453456 0.14946618929453456 0.09048406779766083
0.15147624289613176 0.15147624289613176 0.11728775501251221
0.15342807936069466 0.15342807936069466 0.13827134668827057
0.15417924533112565 0.15417924533112565 0.1448931246995926
0.15326177108141617 0.15326177108141617 0.13457177579402924
0.15112744891153485 0.15112744891153485 0.10872150957584381
0.14884876896598723 0.14884876896598723 0.07180174440145493
0.14745556820403777 0.14745556820403777 0.03164128214120865
0.14737003971268275 0.14737003971268275 0.024925265461206436
0.14828714276420144 0.14828714276420144 0.057195331901311874
0.14947711786382498 0.14947711786382498 0.08295902609825134
0.15022660918685346 0.15022660918685346 0.09620947390794754
0.15015999678077863 0.15015999678077863 0.09562674164772034
0.14935392870032638 0.14935392870032638 0.08210975676774979
0.14825577727355693 0.14825577727355693 0.05851960554718971
0.14742337651455645 0.14742337651455645 0.02978624403476715
0.14720228202854357 0.14720228202854357 0.012849661521613598
0.1475388321538766 0.1475388321538766 0.033601824194192886
0.1480759237140273 0.1480759237140273 0.05247858539223671
0.14843223229041436 0.14843223229041436 0.06236603856086731
0.1484262679246855 0.1484262679246855 0.06270893663167953
0.14811309428337494 0.14811309428337494 0.054868653416633606
0.14768871069114436 0.14768871069114436 0.041624121367931366
0.14736732848410256 0.14736732848410256 0.027742478996515274
0.14727521161332108 0.14727521161332108 0.022140590474009514
0.14738912516728087 0.14738912516728087 0.028299057856202126
0.147567527338252 0.147567527338252 0.03585918992757797
0.14766399630080412 0.14766399630080412 0.03932304307818413
0.14762996884191437 0.14762996884191437 0.03816841170191765
0.14751380281345594 0.14751380281345594 0.03393072634935379
0.14739156892406868 0.14739156892406868 0.028821302577853203
0.14731241650434992 0.14731241650434992 0.02478787675499916
0.14728824613253028 0.14728824613253028 0.02281632460653782
0.1473030887268011 0.1473030887268011 0.02252129837870598
0.1473260690033883 0.1473260690033883 0.022738322615623474
0.1473313421150893 0.1473313421150893 0.022763073444366455
0.14731785728625352 0.14731785728625352 0.022761289030313492
0.14729979245331323 0.14729979245331323 0.022944418713450432
0.14728355006351268 0.14728355006351268 0.022678423672914505
0.14726277913574173 0.14726277913574173 0.020947188138961792
0.14723582709909075 0.14723582709909075 0.01751861907541752
0.14721344147942206 0.14721344147942206 0.013643900863826275
0.14720619215322775 0.14720619215322775 0.011986669152975082
0.14721438631701922 0.14721438631701922 0.013749487698078156
0.14722827943187122 0.14722827943187122 0.016532624140381813
0.14723683335172597 0.14723683335172597 0.018051752820611
0.14722924962501316 0.14722924962501316 0.017246969044208527
0.14720534688591969 0.14720534688591969 0.013951080851256847
0.14717814875212765 0.14717814875212765 0.008999873884022236
0.14716571609208168 0.14716571609208168 0.005472668446600437
0.14717349476847297 0.14717349476847297 0.008019163273274899
0.147191008875322 0.147191008875322 0.011883358471095562
0.14720226815711823 0.14720226815711823 0.013870744965970516
0.14719834852742036 0.14719834852742036 0.01330468151718378
0.1471815527217652 0.1471815527217652 0.0103275952860713
0.14716357241819697 0.14716357241819697 0.005661947187036276
0.14715627365293033 0.14715627365293033 0.00168334087356925
iteration: 1 | epoch: 82 |   loss: 0.147156  |   KL divergence: 0.147156  |  JS divergence: 0.045358
('==== Found maximium gradient [0.40150183, 0.39299017, 0.20886046] of gate '
 'e^[Y1 Z2], e^[Y0 Z2], e^[Y2 Z0] ====')
learning rate =  0.06921153019778761
0.14716271974299544 0.14716271974299544 0.5994136929512024
0.09315919395221806 0.09315919395221806 0.5105252265930176
0.0528848902602731 0.0528848902602731 0.286284863948822
0.036955459404432664 0.036955459404432664 0.23056885600090027
0.026473484684783333 0.026473484684783333 0.19812937080860138
0.020353796688418788 0.020353796688418788 0.17120864987373352
0.018979522707466424 0.018979522707466424 0.1674896627664566
0.020724443416638677 0.020724443416638677 0.18537123501300812
0.022239148079503838 0.022239148079503838 0.19912321865558624
0.0217321026873517 0.0217321026873517 0.19629444181919098
0.020145790246078245 0.020145790246078245 0.18401238322257996
0.018971803648146924 0.018971803648146924 0.17391979694366455
0.018724664814651796 0.018724664814651796 0.17118234932422638
0.0187791174424409 0.0187791174424409 0.17019161581993103
0.018159748583598742 0.018159748583598742 0.16149397194385529
0.016611159642717907 0.016611159642717907 0.14213266968727112
0.0147667500231073 0.0147667500231073 0.11959894001483917
0.013377348927952074 0.013377348927952074 0.10879377275705338
0.01245160151825732 0.01245160151825732 0.1139025166630745
0.011254170931897945 0.011254170931897945 0.11823495477437973
0.009285521466924287 0.009285521466924287 0.10710609704256058
0.007016560737702598 0.007016560737702598 0.0802399069070816
0.005467568223456046 0.005467568223456046 0.050845757126808167
0.005203548296757072 0.005203548296757072 0.04478875920176506
0.0058458009008258505 0.0058458009008258505 0.059776898473501205
0.006578600847496575 0.006578600847496575 0.07046335935592651
0.006957704113868534 0.006957704113868534 0.07261668145656586
0.007106054717507794 0.007106054717507794 0.0733206495642662
0.007259555661281487 0.007259555661281487 0.07934948801994324
0.007306999921425316 0.007306999921425316 0.08700352162122726
0.0068740562338033915 0.0068740562338033915 0.0867457315325737
0.005857956739231951 0.005857956739231951 0.07351712882518768
0.004678868644841055 0.004678868644841055 0.050331536680459976
0.0038886889018578275 0.0038886889018578275 0.02923545613884926
0.0036463213920431394 0.0036463213920431394 0.030036501586437225
0.0036759076124202655 0.0036759076124202655 0.040116649121046066
0.0036684103707819867 0.0036684103707819867 0.042342282831668854
0.0035943355018794266 0.0035943355018794266 0.03709854930639267
0.003607273863827836 0.003607273863827836 0.03319679945707321
0.0037439539929834237 0.0037439539929834237 0.03676370158791542
0.003832782847598456 0.003832782847598456 0.040484052151441574
0.0037183647641325502 0.0037183647641325502 0.037509072571992874
0.0034630796286400636 0.0034630796286400636 0.02961484156548977
0.0032465057745249366 0.0032465057745249366 0.026096422225236893
0.0031440692636177485 0.0031440692636177485 0.031155403703451157
0.0030748457111927133 0.0030748457111927133 0.03596774861216545
0.002946313153510606 0.002946313153510606 0.035087257623672485
0.002770927468204222 0.002770927468204222 0.029465902596712112
0.002625056215140678 0.002625056215140678 0.024102941155433655
0.002537822024414883 0.002537822024414883 0.02299628034234047
0.0024664522482176297 0.0024664522482176297 0.022939153015613556
0.0023750661566024063 0.0023750661566024063 0.019759763032197952
0.00229059341855932 0.00229059341855932 0.014609141275286674
0.0022576251414287776 0.0022576251414287776 0.013674349524080753
0.002269886795166101 0.002269886795166101 0.017643526196479797
0.00227698371509629 0.00227698371509629 0.020462987944483757
0.002244217278783816 0.002244217278783816 0.02049713395535946
0.0021829294150990273 0.0021829294150990273 0.019718093797564507
0.002117066448126136 0.002117066448126136 0.019880857318639755
0.0020466816296716202 0.0020466816296716202 0.019561633467674255
0.0019637287693907296 0.0019637287693907296 0.01663055643439293
0.0018787073106396462 0.0018787073106396462 0.011370646767318249
0.0018161766915477748 0.0018161766915477748 0.007604288402944803
0.001783673970738679 0.001783673970738679 0.009348703548312187
0.0017662515168100608 0.0017662515168100608 0.011798683553934097
0.0017461055441815622 0.0017461055441815622 0.012252124026417732
0.0017224448798170734 0.0017224448798170734 0.011603472754359245
0.0016980118799086621 0.0016980118799086621 0.011249725706875324
0.0016711072099848427 0.0016711072099848427 0.010949160903692245
0.0016368221114432678 0.0016368221114432678 0.009816859848797321
0.0015981952881683648 0.0015981952881683648 0.00831826776266098
0.0015626427098376643 0.0015626427098376643 0.007854471914470196
0.0015330580587973532 0.0015330580587973532 0.008206899277865887
0.0015049045399005488 0.0015049045399005488 0.007961184717714787
0.0014757063006741941 0.0014757063006741941 0.006977304350584745
0.0014462767538268306 0.0014462767538268306 0.006355847232043743
0.0014192124048674334 0.0014192124048674334 0.006488325539976358
0.0013935488882683075 0.0013935488882683075 0.006424222141504288
0.0013696393593766055 0.0013696393593766055 0.005836854688823223
0.0013485809603301334 0.0013485809603301334 0.005484553053975105
0.0013299436498625397 0.0013299436498625397 0.005822960287332535
0.0013124195485937637 0.0013124195485937637 0.00628256518393755
0.0012944015307544262 0.0012944015307544262 0.006471929140388966
0.001275253719891888 0.001275253719891888 0.006472080945968628
0.0012542640419932038 0.0012542640419932038 0.00625626090914011
0.0012323623433045704 0.0012323623433045704 0.005645868368446827
0.0012114850831310216 0.0012114850831310216 0.004794420208781958
iteration: 2 | epoch: 169 |   loss: 0.001211  |   KL divergence: 0.001211  |  JS divergence: 0.000391
('==== Found maximium gradient [0.026788935, 0.022825424, 0.019673042] of gate '
 'e^[X1 Y2], e^[X1 Y0], e^[X0 Y2] ====')
learning rate =  0.004655714034513157
0.0011922931985267017 0.0011922931985267017 0.040540583431720734
0.0009273306414109596 0.0009273306414109596 0.030768772587180138
0.0007201487798986911 0.0007201487798986911 0.021976038813591003
0.0006295097736641903 0.0006295097736641903 0.02139844372868538
0.0005866713938601845 0.0005866713938601845 0.023602213710546494
0.0005754694184031822 0.0005754694184031822 0.027478300034999847
0.000545794262913922 0.000545794262913922 0.029602494090795517
0.0004790953913254327 0.0004790953913254327 0.02884773723781109
0.00039214907208614397 0.00039214907208614397 0.026044659316539764
0.000303250785965041 0.000303250785965041 0.02193169854581356
0.00022245117625422674 0.00022245117625422674 0.016740480437874794
0.00016016818375499866 0.00016016818375499866 0.011470021679997444
0.00012373057566659063 0.00012373057566659063 0.008812041021883488
0.00010722138802192992 0.00010722138802192992 0.010043870657682419
9.718406520531288e-05 9.718406520531288e-05 0.012027466669678688
8.56619234012706e-05 8.56619234012706e-05 0.013093594461679459
7.05325711593582e-05 7.05325711593582e-05 0.013086137361824512
5.0764702713933284e-05 5.0764702713933284e-05 0.011886882595717907
2.88131030344173e-05 2.88131030344173e-05 0.009372154250741005
1.0530567527149864e-05 1.0530567527149864e-05 0.005887895822525024
1.345829451007913e-06 1.345829451007913e-06 0.00245689763687551
iteration: 3 | epoch: 190 |   loss: 0.000001  |   KL divergence: 0.000001  |  JS divergence: 0.000000
('==== Found maximium gradient [0.0011487874, 0.0010793633, 0.0010722183] of '
 'gate e^[Y1 Z2], e^[X1 Y2], e^[Y1 Z0] ====')
learning rate =  0.00022013298028210877
2.2013083101032112e-06 2.2013083101032112e-06 0.003157378640025854
iteration: 4 | epoch: 191 |   loss: 0.000002  |   KL divergence: 0.000002  |  JS divergence: 0.000001
('==== Found maximium gradient [0.0005931563, 0.0005062697, 0.00047542524] of '
 'gate e^[X2 Y0], e^[X2 Y1], e^[Y2 Z1] ====')
Convergence criterion has reached, break the loop!
