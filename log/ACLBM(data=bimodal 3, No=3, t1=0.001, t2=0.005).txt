('==== Found maximium gradient [0.4085174, 0.40851736, 0.17351666] of gate '
 'e^[Y1 Z2], e^[Y2 Z1], e^[X0 Y1] ====')
learning rate =  0.06965447744987646
0.10556071055793984 0.10556071055793984 0.6278089880943298
0.046375028091850484 0.046375028091850484 0.40983664989471436
0.015166165561816582 0.015166165561816582 0.2342360019683838
0.009354757078320154 0.009354757078320154 0.1696893870830536
0.01294787984806079 0.01294787984806079 0.2087993025779724
0.018201041940594 0.018201041940594 0.26759055256843567
0.02206479041930166 0.02206479041930166 0.30306363105773926
0.02362552991973109 0.02362552991973109 0.31142985820770264
0.02244589610975788 0.02244589610975788 0.2955611050128937
0.01796546878779038 0.01796546878779038 0.25454816222190857
0.011311064083084278 0.011311064083084278 0.19141481816768646
0.005193043691646342 0.005193043691646342 0.11534617841243744
0.0019798690377565813 0.0019798690377565813 0.04522135108709335
0.0023515926437905274 0.0023515926437905274 0.06350736320018768
0.005238361625882667 0.005238361625882667 0.12210635840892792
0.008585581735683974 0.008585581735683974 0.16502498090267181
0.010502157859147515 0.010502157859147515 0.1850080043077469
0.01022251766147006 0.01022251766147006 0.18213221430778503
0.008203209618678257 0.008203209618678257 0.16076670587062836
0.0055640344189129415 0.0055640344189129415 0.12786278128623962
0.003409772952439487 0.003409772952439487 0.09207320958375931
0.0023172958097884434 0.0023172958097884434 0.06425373256206512
0.002212228569888876 0.002212228569888876 0.056571029126644135
0.00264340198561157 0.00264340198561157 0.06719043105840683
0.0031779788254832075 0.0031779788254832075 0.08253034949302673
0.003611056020570576 0.003611056020570576 0.09585541486740112
0.003900905853720481 0.003900905853720481 0.10475622117519379
0.0039904307303611376 0.0039904307303611376 0.10721360146999359
0.003746046516117315 0.003746046516117315 0.10109850764274597
0.003097243918276843 0.003097243918276843 0.08525077253580093
0.002215546815543031 0.002215546815543031 0.060594797134399414
0.0014854058966561542 0.0014854058966561542 0.030759455636143684
0.001246672512128667 0.001246672512128667 0.012973408214747906
0.001542761906340148 0.001542761906340148 0.03632190078496933
0.00209193971220861 0.00209193971220861 0.057902637869119644
0.0025055380727108806 0.0025055380727108806 0.07009406387805939
0.002552581877798941 0.002552581877798941 0.07185093313455582
0.002264920381935903 0.002264920381935903 0.06438178569078445
0.001846128367762559 0.001846128367762559 0.05046408623456955
0.001503677108066241 0.001503677108066241 0.03406056389212608
0.0013411542748932993 0.0013411542748932993 0.021425761282444
0.0013478472484574275 0.0013478472484574275 0.021707993000745773
0.0014473969354966854 0.0014473969354966854 0.03017185628414154
0.0015575451501738443 0.0015575451501738443 0.0376238189637661
0.0016289094679021662 0.0016289094679021662 0.04170239716768265
0.0016458510501885975 0.0016458510501885975 0.04231218248605728
0.0016049810264212386 0.0016049810264212386 0.03964361175894737
0.0015054589685793204 0.0015054589685793204 0.03364100679755211
0.0013675248729500362 0.0013675248729500362 0.0243654977530241
0.001246410766264912 0.001246410766264912 0.013009434565901756
0.0012067736979474303 0.0012067736979474303 0.007680654525756836
0.0012664514827439564 0.0012664514827439564 0.0170121006667614
0.0013699867421184896 0.0013699867421184896 0.025666791945695877
0.0014321544877754335 0.0014321544877754335 0.029694629833102226
0.0014095718576236224 0.0014095718576236224 0.028501097112894058
0.001329059741754597 0.001329059741754597 0.02308574505150318
0.001251269986371492 0.001251269986371492 0.0156251173466444
0.0012145625434197781 0.0012145625434197781 0.009491642005741596
0.0012170253790920187 0.0012170253790920187 0.009228301234543324
0.0012374178511389642 0.0012374178511389642 0.012782230041921139
0.0012577483684910354 0.0012577483684910354 0.015995914116501808
0.001269977263741447 0.001269977263741447 0.01789606548845768
0.001270952037788588 0.001270952037788588 0.01821315661072731
0.001257955486494534 0.001257955486494534 0.016703223809599876
0.0012334853998646898 0.0012334853998646898 0.013299347832798958
0.0012071542606682773 0.0012071542606682773 0.008418751880526543
0.0011918010962791428 0.0011918010962791428 0.003961680456995964
iteration: 1 | epoch: 68 |   loss: 0.001192  |   KL divergence: 0.001192  |  JS divergence: 0.000299
('==== Found maximium gradient [0.036236756, 0.03287987, 0.023402268] of gate '
 'e^[Y1 Z0], e^[Y2 Z0], e^[Y0 Z1] ====')
learning rate =  0.006262964464129827
0.0011955963584195648 0.0011955963584195648 0.05458185076713562
0.0007614557498451463 0.0007614557498451463 0.037688929587602615
0.00047243978723299413 0.00047243978723299413 0.018533460795879364
0.0004132223137636769 0.0004132223137636769 0.01363599207252264
0.0004659265961922728 0.0004659265961922728 0.02151167206466198
0.0005011779026047167 0.0005011779026047167 0.02541089430451393
0.0005008491657868252 0.0005008491657868252 0.02566160075366497
0.0004813065272508243 0.0004813065272508243 0.02394835837185383
0.00045007685176491185 0.00045007685176491185 0.02066885679960251
0.0004119073800586126 0.0004119073800586126 0.01586674340069294
0.0003847915854073092 0.0003847915854073092 0.011273306794464588
0.00038164935160320496 0.00038164935160320496 0.010594315826892853
0.00039720454562196955 0.00039720454562196955 0.013553455471992493
0.00041294325683381895 0.00041294325683381895 0.016246216371655464
0.0004155283050516192 0.0004155283050516192 0.017209574580192566
0.0004035608864730978 0.0004035608864730978 0.016512470319867134
0.0003830541964514472 0.0003830541964514472 0.014559685252606869
0.00036000941053650833 0.00036000941053650833 0.011601346544921398
0.0003401721652991597 0.0003401721652991597 0.00792963057756424
0.0003293228469217861 0.0003293228469217861 0.0046470980159938335
iteration: 2 | epoch: 88 |   loss: 0.000329  |   KL divergence: 0.000329  |  JS divergence: 0.000082
('==== Found maximium gradient [0.011937837, 0.011587445, 0.0031083769] of '
 'gate e^[X2 Y0], e^[X1 Y0], e^[Y0 Z1] ====')
learning rate =  0.001954286960101745
0.00033058803645911604 0.00033058803645911604 0.017579862847924232
0.0002873214169105509 0.0002873214169105509 0.011652595363557339
0.00026622223569054454 0.00026622223569054454 0.008861334063112736
0.00026145446554782007 0.00026145446554782007 0.0107888113707304
0.00026231688893395017 0.00026231688893395017 0.013533390127122402
0.00026110332663402814 0.00026110332663402814 0.015000944025814533
0.00025649674329348963 0.00025649674329348963 0.015105502679944038
0.0002492702652402433 0.0002492702652402433 0.014277446083724499
0.0002385348411342651 0.0002385348411342651 0.012666056863963604
0.00022616905254466992 0.00022616905254466992 0.010542264208197594
0.0002144236949679758 0.0002144236949679758 0.00855453684926033
0.00020571674851476806 0.00020571674851476806 0.007340434473007917
0.00019992381080937284 0.00019992381080937284 0.007122572977095842
0.00019517014023015728 0.00019517014023015728 0.00743117555975914
0.00018901781142139523 0.00018901781142139523 0.00761201698333025
0.0001813555176955117 0.0001813555176955117 0.007452911231666803
0.0001723236584789845 0.0001723236584789845 0.00711688632145524
0.0001631715175752712 0.0001631715175752712 0.006761007942259312
0.00015356154733696284 0.00015356154733696284 0.0064369491301476955
0.0001446060994486605 0.0001446060994486605 0.006277905311435461
0.00013644898770946235 0.00013644898770946235 0.006425636354833841
0.00012969469650424726 0.00012969469650424726 0.006846599746495485
0.00012380334603818174 0.00012380334603818174 0.007364435121417046
0.00011839698796444528 0.00011839698796444528 0.00772062735632062
0.00011239964797679321 0.00011239964797679321 0.007707806769758463
0.000105398704814146 0.000105398704814146 0.0073203737847507
9.841408888586363e-05 9.841408888586363e-05 0.006701391655951738
9.165975988816759e-05 9.165975988816759e-05 0.006001319270581007
8.557407538129934e-05 8.557407538129934e-05 0.005347670055925846
8.039876086061038e-05 8.039876086061038e-05 0.00482619134709239
iteration: 3 | epoch: 118 |   loss: 0.000080  |   KL divergence: 0.000080  |  JS divergence: 0.000020
('==== Found maximium gradient [0.0039921403, 0.0012292737, 0.0012148544] of '
 'gate RY[0], e^[X1 Y0], e^[X2 Y0] ====')
learning rate =  0.0005023168944623631
7.533015826479133e-05 7.533015826479133e-05 0.006222846452146769
7.2229806849523e-05 7.2229806849523e-05 0.007715399842709303
6.729520995426999e-05 6.729520995426999e-05 0.007691851817071438
6.371761308014636e-05 6.371761308014636e-05 0.006846619304269552
6.047801658602648e-05 6.047801658602648e-05 0.006230818573385477
5.729375342287794e-05 5.729375342287794e-05 0.0058265733532607555
5.390299670994438e-05 5.390299670994438e-05 0.005573812406510115
5.027109888843884e-05 5.027109888843884e-05 0.005656980909407139
4.75008390941825e-05 4.75008390941825e-05 0.0059882462956011295
4.468852931751449e-05 4.468852931751449e-05 0.006114404648542404
4.185065528319995e-05 4.185065528319995e-05 0.00578793091699481
3.896176861114978e-05 3.896176861114978e-05 0.005200858693569899
3.639147154537077e-05 3.639147154537077e-05 0.0046894787810742855
iteration: 4 | epoch: 131 |   loss: 0.000036  |   KL divergence: 0.000036  |  JS divergence: 0.000009
('==== Found maximium gradient [0.003042668, 0.0006732987, 0.0006197018] of '
 'gate RY[0], e^[Y2 Z1], e^[Y1 Z2] ====')
learning rate =  0.00036688220654433753
3.421273040798555e-05 3.421273040798555e-05 0.005441620014607906
3.1874560118694685e-05 3.1874560118694685e-05 0.006733407266438007
2.8038029189102068e-05 2.8038029189102068e-05 0.005128804128617048
2.5711496906714328e-05 2.5711496906714328e-05 0.0046459101140499115
iteration: 5 | epoch: 135 |   loss: 0.000026  |   KL divergence: 0.000026  |  JS divergence: 0.000006
('==== Found maximium gradient [0.0024935007, 0.00083465345, 0.0008243007] of '
 'gate RY[0], e^[Y2 Z1], e^[Y0 Z1] ====')
learning rate =  0.00031819624676536815
2.3257766872010986e-05 2.3257766872010986e-05 0.005549931898713112
2.0440266777493204e-05 2.0440266777493204e-05 0.005845749285072088
1.7657022562758598e-05 1.7657022562758598e-05 0.004943773150444031
iteration: 6 | epoch: 138 |   loss: 0.000018  |   KL divergence: 0.000018  |  JS divergence: 0.000004
('==== Found maximium gradient [0.0019785762, 0.0003530231, 0.00032577547] of '
 'gate RY[0], e^[X1 Y0], e^[X0 Y2] ====')
learning rate =  0.0002351033364332379
1.4406445554151527e-05 1.4406445554151527e-05 0.004531571641564369
iteration: 7 | epoch: 139 |   loss: 0.000014  |   KL divergence: 0.000014  |  JS divergence: 0.000004
('==== Found maximium gradient [0.0018911064, 0.0007642443, 0.0007334448] of '
 'gate RY[0], e^[Y1 Z0], e^[Y0 Z1] ====')
learning rate =  0.0002502876700363843
1.2431024367199261e-05 1.2431024367199261e-05 0.0050642360001802444
1.0144457043691105e-05 1.0144457043691105e-05 0.005287515465170145
7.367436682931906e-06 7.367436682931906e-06 0.003923957701772451
iteration: 8 | epoch: 142 |   loss: 0.000007  |   KL divergence: 0.000007  |  JS divergence: 0.000002
('==== Found maximium gradient [0.0010562837, 0.00040612332, 0.00038183754] of '
 'gate RY[0], e^[Y2 Z1], e^[Y1 Z2] ====')
learning rate =  0.00013791162680881747
5.38087139938408e-06 5.38087139938408e-06 0.003144961316138506
iteration: 9 | epoch: 143 |   loss: 0.000005  |   KL divergence: 0.000005  |  JS divergence: 0.000001
('==== Found maximium gradient [0.0009254813, 0.0005494394, 0.00053585] of '
 'gate RY[0], e^[Y1 Z2], e^[Y2 Z1] ====')
Convergence criterion has reached, break the loop!
