('==== Found maximium gradient [0.3350717, 0.33507168, 0.33311677] of gate '
 'e^[Y0 Z1], e^[Y1 Z0], e^[X8 Y0] ====')
learning rate =  0.06688426421964463
0.19413122182685094 0.19413122182685094 0.6681920289993286
0.14300160560476935 0.14300160560476935 0.4593413174152374
0.08465998286735847 0.08465998286735847 0.20063690841197968
0.07639265346092626 0.07639265346092626 0.0920703113079071
0.09590338855932767 0.09590338855932767 0.2846418619155884
0.1113737937771837 0.1113737937771837 0.425337016582489
0.11017637771719518 0.11017637771719518 0.44702816009521484
0.09911461093691676 0.09911461093691676 0.37095630168914795
0.0889175727968361 0.0889175727968361 0.2663809061050415
0.08251156209124863 0.08251156209124863 0.176107719540596
0.07860998995983819 0.07860998995983819 0.1159210205078125
0.07696939623485209 0.07696939623485209 0.10417342931032181
0.07818170213865983 0.07818170213865983 0.1378938853740692
0.08181698584501976 0.08181698584501976 0.18185736238956451
0.08573857443933695 0.08573857443933695 0.2139020413160324
0.08724899063146709 0.08724899063146709 0.223414808511734
0.08517630154575574 0.08517630154575574 0.20741719007492065
0.0806751563205738 0.0806751563205738 0.16919046640396118
0.0761495360367369 0.0761495360367369 0.11654791980981827
0.07357273113711914 0.07357273113711914 0.06282708048820496
0.07341267627417464 0.07341267627417464 0.046200837939977646
0.07475594341179051 0.07475594341179051 0.08134949207305908
0.07631262250195037 0.07631262250195037 0.11674551665782928
0.07726682917350698 0.07726682917350698 0.1387755125761032
0.07742857627600219 0.07742857627600219 0.14498822391033173
0.07692779554420781 0.07692779554420781 0.13623760640621185
0.07591376954777904 0.07591376954777904 0.11497073620557785
0.07456802815333065 0.07456802815333065 0.08426796644926071
0.07328904906842193 0.07328904906842193 0.048575952649116516
0.07262348231495905 0.07262348231495905 0.023328520357608795
0.0728761406540904 0.0728761406540904 0.043634120374917984
0.07379152138860379 0.07379152138860379 0.07154905050992966
0.07467116644437918 0.07467116644437918 0.08926643431186676
0.07489174068137483 0.07489174068137483 0.09323019534349442
0.0743594980636359 0.0743594980636359 0.08394639194011688
0.07350016130116427 0.07350016130116427 0.06493999063968658
0.07284317097175384 0.07284317097175384 0.042126771062612534
0.0726254191796552 0.0726254191796552 0.0253622904419899
0.0727307639962223 0.0727307639962223 0.028325587511062622
0.07292694268422319 0.07292694268422319 0.04106571897864342
0.07308750603000905 0.07308750603000905 0.05181537941098213
0.07319401122265176 0.07319401122265176 0.05779699236154556
0.07322107566657499 0.07322107566657499 0.05791233852505684
0.07310411118224289 0.07310411118224289 0.051460765302181244
0.0728341995350773 0.0728341995350773 0.03865376114845276
0.0725363102591116 0.0725363102591116 0.021377181634306908
0.07239389022388293 0.07239389022388293 0.007803769316524267
0.07248026171030321 0.07248026171030321 0.02015702612698078
0.07268731891408609 0.07268731891408609 0.0331551618874073
0.07282841457025999 0.07282841457025999 0.039721112698316574
0.07280410453355655 0.07280410453355655 0.03921773284673691
0.07266310135388426 0.07266310135388426 0.03296917676925659
0.07251882173507398 0.07251882173507398 0.023505885154008865
0.07244072493268856 0.07244072493268856 0.014226838015019894
0.07242673348991503 0.07242673348991503 0.011012423783540726
0.07245206329021829 0.07245206329021829 0.016030821949243546
0.07249840343395358 0.07249840343395358 0.022209513932466507
0.0725454047230725 0.0725454047230725 0.026133909821510315
0.0725572604514539 0.0725572604514539 0.02637285180389881
0.07251215498233332 0.07251215498233332 0.022431593388319016
0.0724342235587945 0.0724342235587945 0.014947625808417797
0.07237921444074291 0.07237921444074291 0.0061074611730873585
0.0723815163235228 0.0723815163235228 0.006365311332046986
0.07242236170914401 0.07242236170914401 0.01320659276098013
0.0724558607596622 0.0724558607596622 0.017331138253211975
0.0724565988604898 0.0724565988604898 0.01807624287903309
0.07243481308117986 0.07243481308117986 0.016116028651595116
0.07241047227687894 0.07241047227687894 0.012568275444209576
0.07239271016565171 0.07239271016565171 0.008510674349963665
0.07238177681940229 0.07238177681940229 0.0055450028739869595
0.07238084985117348 0.07238084985117348 0.006646705791354179
0.07239330759254503 0.07239330759254503 0.009997055865824223
0.07240886199529178 0.07240886199529178 0.012400325387716293
0.07241193697878054 0.07241193697878054 0.01253466121852398
0.07239699118367275 0.07239699118367275 0.010142968967556953
0.07237723996866027 0.07237723996866027 0.005880274344235659
0.07236866687357905 0.07236866687357905 0.0017154926899820566
iteration: 1 | epoch: 78 |   loss: 0.072369  |   KL divergence: 0.072369  |  JS divergence: 0.019971
('==== Found maximium gradient [0.24210617, 0.2415005, 0.16688485] of gate '
 'e^[Y1 Z2], e^[Y0 Z2], e^[Y2 Z1] ====')
learning rate =  0.04393759993323979
0.07237481389944893 0.07237481389944893 0.38053566217422485
0.052096704318109116 0.052096704318109116 0.29126015305519104
0.035944398021889735 0.035944398021889735 0.1407308578491211
0.033598811847692624 0.033598811847692624 0.12691433727741241
0.03475421874514092 0.03475421874514092 0.15615998208522797
0.03536698782466981 0.03536698782466981 0.17247405648231506
0.03557970185742608 0.03557970185742608 0.18575194478034973
0.035160569122924826 0.035160569122924826 0.1886451691389084
0.03333567968992777 0.03333567968992777 0.1598663032054901
0.031239832238481716 0.031239832238481716 0.11079524457454681
0.03048527488710104 0.03048527488710104 0.08279512822628021
0.031059631133476335 0.031059631133476335 0.09500285983085632
0.031855919392422806 0.031855919392422806 0.11243918538093567
0.03215328089375665 0.03215328089375665 0.11845920234918594
0.03198522049104167 0.03198522049104167 0.11605264246463776
0.03157036591979918 0.03157036591979918 0.11107344180345535
0.03082478153057007 0.03082478153057007 0.10258115828037262
0.02964043355481996 0.02964043355481996 0.08483482152223587
0.02835980048912847 0.02835980048912847 0.05652506276965141
0.02757871298672518 0.02757871298672518 0.024607844650745392
0.027592885499867825 0.027592885499867825 0.023593122139573097
0.028158178256593232 0.028158178256593232 0.04765667766332626
0.028781900400834788 0.028781900400834788 0.06435661762952805
0.029152965880702472 0.029152965880702472 0.07230925559997559
0.029247454535191696 0.029247454535191696 0.07450118660926819
0.029139597258512152 0.029139597258512152 0.07322239875793457
0.02884389444695942 0.02884389444695942 0.06742900609970093
0.028370450321788598 0.028370450321788598 0.054625220596790314
0.027875258325451138 0.027875258325451138 0.035597048699855804
0.027591043962978273 0.027591043962978273 0.01968587003648281
0.027605571074300585 0.027605571074300585 0.026369377970695496
0.027780372205397787 0.027780372205397787 0.03960214927792549
0.027911923083104896 0.027911923083104896 0.046011440455913544
0.027922000572013682 0.027922000572013682 0.044900912791490555
0.027872519059671322 0.027872519059671322 0.03989085555076599
0.027831620968904305 0.027831620968904305 0.035325318574905396
0.027785571759997685 0.027785571759997685 0.03187885135412216
0.027707172961712247 0.027707172961712247 0.02741408906877041
0.02763601516127125 0.02763601516127125 0.023184049874544144
0.027629194340529118 0.027629194340529118 0.023977654054760933
0.027670770667668473 0.027670770667668473 0.028309276327490807
0.027680501243823406 0.027680501243823406 0.030263248831033707
0.027619693645217918 0.027619693645217918 0.02756357565522194
0.027530322311269614 0.027530322311269614 0.02189331129193306
0.02747417775565483 0.02747417775565483 0.016948573291301727
0.027463051390311518 0.027463051390311518 0.014805974438786507
0.027474028961850916 0.027474028961850916 0.014332925900816917
0.027496074399422442 0.027496074399422442 0.015677226707339287
0.027530164904458195 0.027530164904458195 0.019277062267065048
0.027560232288034926 0.027560232288034926 0.022608639672398567
0.027555066984512804 0.027555066984512804 0.022884555160999298
0.02750959794999754 0.02750959794999754 0.019174747169017792
0.027452878571131864 0.027452878571131864 0.012713998556137085
0.02741975481528243 0.02741975481528243 0.007078918162733316
0.027415377239210243 0.027415377239210243 0.007424121722579002
0.02742552227770936 0.02742552227770936 0.010103845037519932
0.027439470104673257 0.027439470104673257 0.011887304484844208
0.027453678131255474 0.027453678131255474 0.013111365027725697
0.027461654960223292 0.027461654960223292 0.013532424345612526
0.027454978391155217 0.027454978391155217 0.01232239231467247
0.027437368817981415 0.027437368817981415 0.009566178545355797
0.02742294723798496 0.02742294723798496 0.006937088910490274
0.027419292673691028 0.027419292673691028 0.006554529070854187
0.02742046077940112 0.02742046077940112 0.007474033161997795
0.027420891624849383 0.027420891624849383 0.008057215251028538
0.02742024152193049 0.02742024152193049 0.008243647404015064
0.027418319610005373 0.027418319610005373 0.008043725974857807
0.027415800867930024 0.027415800867930024 0.007045910228043795
0.02741212686877116 0.02741212686877116 0.00534056918695569
0.027411454369448127 0.027411454369448127 0.0044503905810415745
iteration: 2 | epoch: 148 |   loss: 0.027411  |   KL divergence: 0.027411  |  JS divergence: 0.007412
('==== Found maximium gradient [0.13553761, 0.13533814, 0.08347149] of gate '
 'e^[Y0 Z3], e^[Y1 Z3], e^[Y3 Z0] ====')
learning rate =  0.024125877651527735
0.02741377631128788 0.02741377631128788 0.20900966227054596
0.02343165827159676 0.02343165827159676 0.1695646494626999
0.0169934383998349 0.0169934383998349 0.08621925115585327
0.01577936195998782 0.01577936195998782 0.06929576396942139
0.01716648786045161 0.01716648786045161 0.11269916594028473
0.017440638045183258 0.017440638045183258 0.12107054889202118
0.01668237183804473 0.01668237183804473 0.10386386513710022
0.01602216472030086 0.01602216472030086 0.08313704282045364
0.015967922654826924 0.015967922654826924 0.07685765624046326
0.016130146524090772 0.016130146524090772 0.08053239434957504
0.0159670672710163 0.0159670672710163 0.07893379777669907
0.01551057398666066 0.01551057398666066 0.06834243983030319
0.015137116340740869 0.015137116340740869 0.05493389070034027
0.015103246506646961 0.015103246506646961 0.04986543208360672
0.015327294818909484 0.015327294818909484 0.0565631203353405
0.015487934189811333 0.015487934189811333 0.06391938775777817
0.015329952065329128 0.015329952065329128 0.06292135268449783
0.014895836694601282 0.014895836694601282 0.05256551504135132
0.014444704785511764 0.014444704785511764 0.037296656519174576
0.014205532050883951 0.014205532050883951 0.025132331997156143
0.014226609736396204 0.014226609736396204 0.025220727548003197
0.01439649391669076 0.01439649391669076 0.032625872641801834
0.014555641555766788 0.014555641555766788 0.03825693577528
0.014617336113197002 0.014617336113197002 0.039955977350473404
0.014595056350422818 0.014595056350422818 0.03894706815481186
0.014533208413823604 0.014533208413823604 0.03705691546201706
0.014450730998300658 0.014450730998300658 0.03489882871508598
0.014346050626979025 0.014346050626979025 0.03174315020442009
0.014230955103103134 0.014230955103103134 0.026782453060150146
0.014138459627581191 0.014138459627581191 0.020687099546194077
0.01410414113434613 0.01410414113434613 0.017026878893375397
0.014132421500379216 0.014132421500379216 0.019307104870676994
0.014188623743126596 0.014188623743126596 0.023903323337435722
0.01422299216724109 0.01422299216724109 0.02643796242773533
0.014212260115843159 0.014212260115843159 0.025623340159654617
0.014170939974150925 0.014170939974150925 0.022164640948176384
0.014129595411842421 0.014129595411842421 0.017957735806703568
0.014108691998096796 0.014108691998096796 0.015400692820549011
0.014105687601947787 0.014105687601947787 0.015518642030656338
0.014105699587648582 0.014105699587648582 0.016766982153058052
0.014097956797802217 0.014097956797802217 0.017397692427039146
0.014084568560594828 0.014084568560594828 0.016804516315460205
0.014071175430976971 0.014071175430976971 0.015188389457762241
0.014059891689798099 0.014059891689798099 0.013089705258607864
0.014050928577816575 0.014050928577816575 0.011043623089790344
0.014044464882106426 0.014044464882106426 0.009606380946934223
0.014043989844769133 0.014043989844769133 0.009532060474157333
0.014049971511463635 0.014049971511463635 0.010874392464756966
0.014057657162512602 0.014057657162512602 0.01238574180752039
0.01405917926358215 0.01405917926358215 0.012749066576361656
0.014049787608414153 0.014049787608414153 0.011441577225923538
0.014033874660424068 0.014033874660424068 0.008822275325655937
0.014020274334196454 0.014020274334196454 0.0060021220706403255
0.014015079357463382 0.014015079357463382 0.004837155807763338
iteration: 3 | epoch: 202 |   loss: 0.014015  |   KL divergence: 0.014015  |  JS divergence: 0.003771
('==== Found maximium gradient [0.07180044, 0.07125441, 0.041734163] of gate '
 'e^[Y0 Z4], e^[Y1 Z4], e^[Y4 Z0] ====')
learning rate =  0.012635523237726027
0.01401731557980416 0.01401731557980416 0.10958414524793625
0.013134375762208425 0.013134375762208425 0.09201695770025253
0.011187203976839505 0.011187203976839505 0.044249001890420914
0.010961376838952116 0.010961376838952116 0.03957441821694374
0.011379365399817525 0.011379365399817525 0.05933929979801178
0.011348538481910132 0.011348538481910132 0.060490842908620834
0.011068956912020008 0.011068956912020008 0.051012102514505386
0.010916559824174622 0.010916559824174622 0.04281589388847351
0.010973810223679976 0.010973810223679976 0.04262813180685043
0.011053562698697159 0.011053562698697159 0.04455797001719475
0.010998240857510808 0.010998240857510808 0.04222165793180466
0.010845814027658055 0.010845814027658055 0.03582027181982994
0.010720219062544558 0.010720219062544558 0.029511254280805588
0.010696259946492745 0.010696259946492745 0.028321797028183937
0.010745091347409535 0.010745091347409535 0.0316825695335865
0.010774417764797338 0.010774417764797338 0.03387098386883736
0.010724774686115027 0.010724774686115027 0.031504254788160324
0.010622396194360244 0.010622396194360244 0.025116143748164177
0.010534993781784057 0.010534993781784057 0.01831151731312275
0.010502487090986728 0.010502487090986728 0.016274496912956238
0.010510395011433044 0.010510395011433044 0.018709134310483932
0.010520855709862402 0.010520855709862402 0.020518813282251358
0.010515644445086826 0.010515644445086826 0.019747111946344376
0.010507237611118533 0.010507237611118533 0.017453700304031372
0.010513158981638564 0.010513158981638564 0.016185402870178223
0.010531611214605744 0.010531611214605744 0.017103614285588264
0.010540537829391863 0.010540537829391863 0.0183024313300848
0.010522429664128061 0.010522429664128061 0.017711829394102097
0.010480272719774702 0.010480272719774702 0.014837364666163921
0.010437261212933181 0.010437261212933181 0.010668118484318256
0.01041289289853715 0.01041289289853715 0.00765133835375309
0.010412050849281516 0.010412050849281516 0.008159151300787926
0.010424161080624411 0.010424161080624411 0.01006853673607111
0.010435958400220682 0.010435958400220682 0.011097257025539875
0.01044280885667782 0.01044280885667782 0.011062446050345898
0.010443890167127224 0.010443890167127224 0.010625293478369713
0.01044089943793101 0.01044089943793101 0.010214798152446747
0.010432624331468857 0.010432624331468857 0.009593434631824493
0.01042069923242277 0.01042069923242277 0.008457948453724384
0.010410396797998593 0.010410396797998593 0.0071008396334946156
0.010406044224303742 0.010406044224303742 0.006366784684360027
0.01040728994555028 0.01040728994555028 0.006610397715121508
0.010409338357106526 0.010409338357106526 0.00704209366813302
0.010409093044955514 0.010409093044955514 0.0068981884978711605
0.010406163622703201 0.010406163622703201 0.006144365295767784
0.010403359227867212 0.010403359227867212 0.005377631634473801
0.0104026676318227 0.0104026676318227 0.0052027921192348
0.010402301264581621 0.010402301264581621 0.0053507001139223576
0.010401456846024244 0.010401456846024244 0.005209420341998339
0.01039992531928622 0.01039992531928622 0.004792662337422371
iteration: 4 | epoch: 252 |   loss: 0.010400  |   KL divergence: 0.010400  |  JS divergence: 0.002792
('==== Found maximium gradient [0.040227447, 0.03673991, 0.03640147] of gate '
 'e^[X1 Y2], e^[Y0 Z5], e^[Y1 Z5] ====')
learning rate =  0.0075658314708798875
0.010399319277567638 0.010399319277567638 0.06568630784749985
0.010069575522822614 0.010069575522822614 0.06128359213471413
0.009168829387113615 0.009168829387113615 0.03789845108985901
0.008843285298824543 0.008843285298824543 0.03244277089834213
0.008777491571769715 0.008777491571769715 0.03947928547859192
0.008599476036460607 0.008599476036460607 0.04045182839035988
0.008333944821129884 0.008333944821129884 0.037398774176836014
0.008084480101597532 0.008084480101597532 0.03541822358965874
0.00787659829613901 0.00787659829613901 0.03621157631278038
0.007645347416386397 0.007645347416386397 0.03660067543387413
0.00735071904296774 0.00735071904296774 0.03401237726211548
0.007031007948883912 0.007031007948883912 0.029132088646292686
0.006751587920243158 0.006751587920243158 0.02506369724869728
0.006547054807481528 0.006547054807481528 0.024780910462141037
0.006399345528058411 0.006399345528058411 0.027303611859679222
0.006265908984669508 0.006265908984669508 0.029356716200709343
0.006119114779048321 0.006119114779048321 0.02915777452290058
0.005961199342613312 0.005961199342613312 0.026913968846201897
0.005809564820563449 0.005809564820563449 0.023904800415039062
0.005675142672925598 0.005675142672925598 0.021419910714030266
0.005555440828720423 0.005555440828720423 0.01975344493985176
0.0054410145780476715 0.0054410145780476715 0.01806381158530712
0.005328557100026592 0.005328557100026592 0.015602889470756054
0.005224837456057591 0.005224837456057591 0.012663600035011768
0.005139530819386234 0.005139530819386234 0.010664866305887699
0.005074560838933814 0.005074560838933814 0.010906914249062538
0.005023792445748267 0.005023792445748267 0.012332986108958721
0.0049755082702882 0.0049755082702882 0.013114883564412594
0.004925804731341224 0.004925804731341224 0.012546630576252937
0.0048752215423954505 0.0048752215423954505 0.011037427932024002
0.00483087621305901 0.00483087621305901 0.009655033238232136
0.004796332216024459 0.004796332216024459 0.00914794858545065
0.004769441575759625 0.004769441575759625 0.0090448297560215
0.004744531382974279 0.004744531382974279 0.008385026827454567
0.004721866345398292 0.004721866345398292 0.0069036963395774364
0.004703950108006832 0.004703950108006832 0.00538134528324008
0.004693896250159766 0.004693896250159766 0.005253127310425043
0.004690161969337753 0.004690161969337753 0.006284971721470356
0.0046873388401573145 0.0046873388401573145 0.00690692150965333
0.004682082205894889 0.004682082205894889 0.006488816812634468
0.004674590748604492 0.004674590748604492 0.0053529818542301655
0.004670017117018085 0.004670017117018085 0.004439122974872589
iteration: 5 | epoch: 294 |   loss: 0.004670  |   KL divergence: 0.004670  |  JS divergence: 0.001261
('==== Found maximium gradient [0.01922817, 0.01819839, 0.018079266] of gate '
 'e^[X0 Y3], e^[Y1 Z6], e^[Y0 Z6] ====')
learning rate =  0.0037018259433409407
0.004668018417801569 0.004668018417801569 0.03235245496034622
0.004581195327371331 0.004581195327371331 0.031639423221349716
0.004362175627764969 0.004362175627764969 0.018931392580270767
0.004296045964255091 0.004296045964255091 0.015856945887207985
0.004281133417060367 0.004281133417060367 0.018329866230487823
0.004238532242165732 0.004238532242165732 0.01823241263628006
0.004189947388072169 0.004189947388072169 0.017692361027002335
0.004154727447318535 0.004154727447318535 0.018706442788243294
0.004123443544886982 0.004123443544886982 0.020203452557325363
0.004072948142156856 0.004072948142156856 0.020029623061418533
0.003998684820584065 0.003998684820584065 0.017663247883319855
0.003919322163131394 0.003919322163131394 0.0141801992431283
0.0038528884509347098 0.0038528884509347098 0.011492335237562656
0.003803695091411744 0.003803695091411744 0.010933205485343933
0.0037635518924765904 0.0037635518924765904 0.011535081081092358
0.0037233885307398854 0.0037233885307398854 0.011861997656524181
0.0036824312130983765 0.0036824312130983765 0.011752039194107056
0.003643598353555558 0.003643598353555558 0.01175055094063282
0.0036086819531525177 0.0036086819531525177 0.012084254994988441
0.0035746956341330583 0.0035746956341330583 0.012307663448154926
0.003537907729492321 0.003537907729492321 0.01186389196664095
0.0034980008816549047 0.0034980008816549047 0.010625364258885384
0.0034589825140993627 0.0034589825140993627 0.008955344557762146
0.0034240078217255806 0.0034240078217255806 0.007479208987206221
0.0033941670344034354 0.0033941670344034354 0.006686271168291569
0.0033690603367846315 0.0033690603367846315 0.006535589694976807
0.003346298968360948 0.003346298968360948 0.0066319177858531475
0.0033248547799017317 0.0033248547799017317 0.00670230807736516
0.003303440072674733 0.003303440072674733 0.006686278618872166
0.0032827585077820206 0.0032827585077820206 0.006598510779440403
0.0032629593031260652 0.0032629593031260652 0.006415132898837328
0.0032433241542915783 0.0032433241542915783 0.006058319006115198
0.0032240483036978214 0.0032240483036978214 0.005460509564727545
0.0032053119674372074 0.0032053119674372074 0.004670427180826664
iteration: 6 | epoch: 328 |   loss: 0.003205  |   KL divergence: 0.003205  |  JS divergence: 0.000874
('==== Found maximium gradient [0.012274351, 0.011951714, 0.00920786] of gate '
 'e^[X0 Y4], e^[X1 Y4], e^[Y1 Z7] ====')
learning rate =  0.002245849049766032
0.0031879474478376333 0.0031879474478376333 0.019847236573696136
0.003147943890133921 0.003147943890133921 0.01866719499230385
0.003062103551255967 0.003062103551255967 0.011631726287305355
0.0030392939314035426 0.0030392939314035426 0.012960333377122879
0.003026592624987055 0.003026592624987055 0.015598624013364315
0.0029975486225051508 0.0029975486225051508 0.015815069898962975
0.00296344968676496 0.00296344968676496 0.014840045012533665
0.0029346674318825854 0.0029346674318825854 0.013899000361561775
0.0029090980964453354 0.0029090980964453354 0.013182624243199825
0.0028806233440769333 0.0028806233440769333 0.012010164558887482
0.0028469312559290284 0.0028469312559290284 0.01033900398761034
0.00281498560359126 0.00281498560359126 0.009016119875013828
0.002788308201482701 0.002788308201482701 0.008830967359244823
0.0027652586175628416 0.0027652586175628416 0.009365944191813469
0.002740959221102828 0.002740959221102828 0.009553980082273483
0.002714617154173718 0.002714617154173718 0.008908712305128574
0.0026866511822502296 0.0026866511822502296 0.007692852523177862
0.0026612130702359294 0.0026612130702359294 0.00663549592718482
0.0026400201968376294 0.0026400201968376294 0.006390640512108803
0.0026212791459231943 0.0026212791459231943 0.006770167499780655
0.0026031594681468944 0.0026031594681468944 0.007110518403351307
0.0025852636884428175 0.0025852636884428175 0.007121995557099581
0.002566410520394886 0.002566410520394886 0.006927308160811663
0.002549517872851125 0.002549517872851125 0.00671369070187211
0.0025339599715248073 0.0025339599715248073 0.006444279104471207
0.0025190314236995686 0.0025190314236995686 0.005915657617151737
0.0025035122742611904 0.0025035122742611904 0.005050089210271835
0.002490089342862888 0.002490089342862888 0.004092456307262182
iteration: 7 | epoch: 356 |   loss: 0.002490  |   KL divergence: 0.002490  |  JS divergence: 0.000684
('==== Found maximium gradient [0.008835424, 0.00744441, 0.005694922] of gate '
 'e^[X1 Y5], e^[X0 Y5], e^[Y3 Z2] ====')
learning rate =  0.0014873515424748303
0.0024776467341195187 0.0024776467341195187 0.013362633995711803
0.0024504055418127146 0.0024504055418127146 0.011997492983937263
0.002414209603637472 0.002414209603637472 0.008678426966071129
0.002399272762923121 0.002399272762923121 0.009146482683718204
0.0023875949371039568 0.0023875949371039568 0.010491115972399712
0.002373713283040968 0.002373713283040968 0.011261072009801865
0.0023585452360369856 0.0023585452360369856 0.01128333155065775
0.0023448293174182042 0.0023448293174182042 0.010968312621116638
0.0023305103939089407 0.0023305103939089407 0.010248408652842045
0.0023159663967435087 0.0023159663967435087 0.009174388833343983
0.002299874778371552 0.002299874778371552 0.007990150712430477
0.002285277453052634 0.002285277453052634 0.006967246066778898
0.0022714708547871866 0.0022714708547871866 0.0064043267630040646
0.002259577250806987 0.002259577250806987 0.006310893688350916
0.00224811635479701 0.00224811635479701 0.006353258155286312
0.00223518910875515 0.00223518910875515 0.0061957696452736855
0.0022226258339186128 0.0022226258339186128 0.005748165771365166
0.0022106855705679926 0.0022106855705679926 0.005255565978586674
0.002199537548232367 0.002199537548232367 0.005030985455960035
0.0021892602863191183 0.0021892602863191183 0.005062832031399012
0.002179679754882524 0.002179679754882524 0.00517306849360466
0.002170080369162684 0.002170080369162684 0.0052819824777543545
0.002160521694046853 0.002160521694046853 0.005323559977114201
0.002151587740757197 0.002151587740757197 0.0052419076673686504
0.002143132637731697 0.002143132637731697 0.005065018776804209
0.0021341611939808486 0.0021341611939808486 0.004815381020307541
iteration: 8 | epoch: 382 |   loss: 0.002134  |   KL divergence: 0.002134  |  JS divergence: 0.000588
('==== Found maximium gradient [0.007208347, 0.0069525125, 0.005490344] of '
 'gate e^[Y4 Z3], e^[Y3 Z4], e^[Y4 Z2] ====')
learning rate =  0.0013187952742738979
0.0021268414958293385 0.0021268414958293385 0.012270606122910976
0.0021186322517913634 0.0021186322517913634 0.012532586231827736
0.0020796908079707497 0.0020796908079707497 0.006574850529432297
0.0020679393216817953 0.0020679393216817953 0.006060393527150154
0.002064463454951393 0.002064463454951393 0.007986725308001041
0.002055441560684213 0.002055441560684213 0.008165035396814346
0.0020441709756584803 0.0020441709756584803 0.007345246616750956
0.002034544994793679 0.002034544994793679 0.0068274145014584064
0.0020283223750163196 0.0020283223750163196 0.006909669376909733
0.0020205172091313896 0.0020205172091313896 0.00677984394133091
0.0020106135371905087 0.0020106135371905087 0.0059843226335942745
0.0020008959781898004 0.0020008959781898004 0.004852896090596914
iteration: 9 | epoch: 394 |   loss: 0.002001  |   KL divergence: 0.002001  |  JS divergence: 0.000555
('==== Found maximium gradient [0.005336029, 0.005196326, 0.0045960937] of '
 'gate e^[Y0 Z7], e^[Y7 Z0], e^[Y1 Z8] ====')
learning rate =  0.0010106042948468533
0.0019923416300241118 0.0019923416300241118 0.009696667082607746
0.0019799557399556634 0.0019799557399556634 0.009125570766627789
0.001960896552531002 0.001960896552531002 0.005495346616953611
0.0019525621499313563 0.0019525621499313563 0.004496437963098288
iteration: 10 | epoch: 398 |   loss: 0.001953  |   KL divergence: 0.001953  |  JS divergence: 0.000541
('==== Found maximium gradient [0.004589418, 0.003982962, 0.0037383987] of '
 'gate e^[X1 Y6], e^[X0 Y6], e^[X1 Y3] ====')
learning rate =  0.0008238314182745015
0.0019490325175932624 0.0019490325175932624 0.008864999748766422
0.0019326411177623226 0.0019326411177623226 0.00683643389493227
0.0019235244561178018 0.0019235244561178018 0.006551513448357582
0.0019136424206853968 0.0019136424206853968 0.006040539592504501
0.0019057690583806476 0.0019057690583806476 0.006538030691444874
0.001899200027290418 0.001899200027290418 0.007392568979412317
0.0018927724518821353 0.0018927724518821353 0.007765955291688442
0.0018847029433687146 0.0018847029433687146 0.007573597598820925
0.001876390600661928 0.001876390600661928 0.007029413711279631
0.0018679319785489025 0.0018679319785489025 0.0063537717796862125
0.0018597783101152432 0.0018597783101152432 0.005709680262953043
0.0018527958117341274 0.0018527958117341274 0.00518412608653307
0.0018454412036063605 0.0018454412036063605 0.004823917523026466
iteration: 11 | epoch: 411 |   loss: 0.001845  |   KL divergence: 0.001845  |  JS divergence: 0.000511
('==== Found maximium gradient [0.003581576, 0.0028961443, 0.0028075133] of '
 'gate e^[X2 Y3], e^[X0 Y1], e^[Y0 Z1] ====')
learning rate =  0.0006228693551509472
0.0018389491543125522 0.0018389491543125522 0.007127189543098211
0.001831833619808715 0.001831833619808715 0.0073503670282661915
0.001819975252056361 0.001819975252056361 0.006536541972309351
0.00181056753351242 0.00181056753351242 0.006493189837783575
0.0018019263721687968 0.0018019263721687968 0.0063978200778365135
0.001793047609720174 0.001793047609720174 0.0060700103640556335
0.001784234237905529 0.001784234237905529 0.0058713676407933235
0.001776412338054735 0.001776412338054735 0.005918696988373995
0.0017688453874526889 0.0017688453874526889 0.005997703410685062
0.001761388682675713 0.001761388682675713 0.0059149847365915775
0.0017536489825968737 0.0017536489825968737 0.0057267118245363235
0.0017465301753769298 0.0017465301753769298 0.005549880210310221
0.0017394150159164828 0.0017394150159164828 0.005400446709245443
0.001733087499076332 0.001733087499076332 0.005225903354585171
0.001725800976760422 0.001725800976760422 0.005003171972930431
0.0017196737682353466 0.0017196737682353466 0.004774049855768681
iteration: 12 | epoch: 427 |   loss: 0.001720  |   KL divergence: 0.001720  |  JS divergence: 0.000480
('==== Found maximium gradient [0.0029515151, 0.002900128, 0.0026463624] of '
 'gate e^[Y0 Z4], e^[Y2 Z0], e^[Y8 Z0] ====')
learning rate =  0.0005671615528169646
0.0017133954558967118 0.0017133954558967118 0.006738563999533653
0.0017054242312184516 0.0017054242312184516 0.007362214848399162
0.001694325586283406 0.001694325586283406 0.006195794325321913
0.001685028675366304 0.001685028675366304 0.005809744819998741
0.0016773187748378162 0.0016773187748378162 0.005832166410982609
0.0016685465207004076 0.0016685465207004076 0.005678500514477491
0.0016601754441729869 0.0016601754441729869 0.005622310563921928
0.0016525524597861716 0.0016525524597861716 0.0058091627433896065
0.0016447336574853093 0.0016447336574853093 0.0060586282052099705
0.0016381501634984065 0.0016381501634984065 0.006116944365203381
0.0016302669182770443 0.0016302669182770443 0.0059648859314620495
0.0016227404336830525 0.0016227404336830525 0.005772505886852741
0.001614980167134566 0.001614980167134566 0.00569175137206912
0.0016079081567560798 0.0016079081567560798 0.005725192371755838
0.0016010347852305905 0.0016010347852305905 0.005764210596680641
0.0015939684691553871 0.0015939684691553871 0.005749060306698084
0.0015865386410563779 0.0015865386410563779 0.0057198889553546906
0.00157966393863258 0.00157966393863258 0.005727033130824566
0.001573364689885053 0.001573364689885053 0.005761617794632912
0.0015663563837544283 0.0015663563837544283 0.005780632607638836
0.0015597006881181042 0.0015597006881181042 0.00576024828478694
0.001553028592245742 0.001553028592245742 0.005710291210561991
0.0015461980701862931 0.0015461980701862931 0.005654509644955397
0.0015395623963533633 0.0015395623963533633 0.005608175415545702
0.0015330861863009212 0.0015330861863009212 0.005578643176704645
0.0015266854744155288 0.0015266854744155288 0.00557112880051136
0.0015198402545423782 0.0015198402545423782 0.005585219711065292
0.0015132884091330551 0.0015132884091330551 0.005609201733022928
0.001506668195917142 0.001506668195917142 0.005623730830848217
0.0015006883413571166 0.0015006883413571166 0.005618809722363949
0.0014939861123140397 0.0014939861123140397 0.005599715281277895
0.0014872265073845486 0.0014872265073845486 0.0055785831063985825
0.0014816895460125462 0.0014816895460125462 0.005564377177506685
0.0014749628342570842 0.0014749628342570842 0.005555114708840847
0.0014690732878999422 0.0014690732878999422 0.00554285105317831
0.001462133751418071 0.001462133751418071 0.0055214897729456425
0.0014563968129469814 0.0014563968129469814 0.005491551011800766
0.0014496603614326715 0.0014496603614326715 0.0054589868523180485
0.001443906586212848 0.001443906586212848 0.005430892575532198
0.0014373195248125392 0.0014373195248125392 0.005414131097495556
0.0014307976777548524 0.0014307976777548524 0.005411954130977392
0.001424876201595142 0.001424876201595142 0.005419962108135223
0.0014190676819616733 0.0014190676819616733 0.005426672287285328
0.001412713766281668 0.001412713766281668 0.005420705769211054
0.0014065948722682478 0.0014065948722682478 0.005400644615292549
0.0014005946192096686 0.0014005946192096686 0.005373603198677301
0.0013939904069499255 0.0013939904069499255 0.0053495061583817005
0.0013877635203860884 0.0013877635203860884 0.005333170294761658
0.0013821514190229633 0.0013821514190229633 0.005324015859514475
0.0013768209269451794 0.0013768209269451794 0.0053182486444711685
0.0013698221065393398 0.0013698221065393398 0.005312309134751558
0.0013646099711579218 0.0013646099711579218 0.005304032936692238
0.0013585088706586225 0.0013585088706586225 0.005294856149703264
0.0013519358992019274 0.0013519358992019274 0.005287347361445427
0.001346793103271761 0.001346793103271761 0.005281270015984774
0.0013408683898456849 0.0013408683898456849 0.005273294635117054
0.001334850669878213 0.001334850669878213 0.005261148326098919
0.001328813009896005 0.001328813009896005 0.005247575230896473
0.0013235325928963341 0.0013235325928963341 0.00523681053891778
0.0013174441732810248 0.0013174441732810248 0.005229745991528034
0.0013114312744016213 0.0013114312744016213 0.005224697757512331
0.0013064281310544298 0.0013064281310544298 0.005219163838773966
0.0013004763032060714 0.0013004763032060714 0.005211394280195236
0.0012946521136274222 0.0012946521136274222 0.005201423540711403
0.001289079276414874 0.001289079276414874 0.005191023461520672
0.001283015468028943 0.001283015468028943 0.005182317923754454
0.001277513319789677 0.001277513319789677 0.005175618454813957
0.0012713137328019515 0.0012713137328019515 0.005168922245502472
0.0012663325106340893 0.0012663325106340893 0.005160070024430752
0.001260683921074558 0.001260683921074558 0.005149361677467823
0.0012546526978970633 0.0012546526978970633 0.005139942280948162
0.0012487411876110528 0.0012487411876110528 0.005133403930813074
0.0012434837112548394 0.0012434837112548394 0.0051290979608893394
0.001238090441840531 0.001238090441840531 0.0051245298236608505
0.0012326965764001383 0.0012326965764001383 0.005118050612509251
0.0012272565792660621 0.0012272565792660621 0.00510950805619359
0.001221391439299576 0.001221391439299576 0.005100791342556477
0.0012163659741387918 0.0012163659741387918 0.005094424355775118
0.0012106864952845266 0.0012106864952845266 0.005090968683362007
0.001205254772520378 0.001205254772520378 0.0050880261696875095
0.001200013502408557 0.001200013502408557 0.00508233904838562
0.001194717592174981 0.001194717592174981 0.005073368549346924
0.0011888309131420425 0.0011888309131420425 0.005063414108008146
0.00118361783723903 0.00118361783723903 0.005055199842900038
0.0011788509805876549 0.0011788509805876549 0.005049449857324362
0.0011729144579574779 0.0011729144579574779 0.005044383462518454
0.0011683576726747155 0.0011683576726747155 0.00503841508179903
0.0011629952549334288 0.0011629952549334288 0.0050309402868151665
0.0011575013436880893 0.0011575013436880893 0.005022724624723196
0.001152396278243472 0.001152396278243472 0.005015083588659763
0.0011474783284925792 0.0011474783284925792 0.0050081065855920315
0.001142133334404515 0.001142133334404515 0.005000756122171879
0.0011371990692907387 0.0011371990692907387 0.004992444533854723
iteration: 13 | epoch: 520 |   loss: 0.001137  |   KL divergence: 0.001137  |  JS divergence: 0.000317
('==== Found maximium gradient [0.00283004, 0.002504311, 0.0024539407] of gate '
 'e^[X4 Y3], e^[Y2 Z0], e^[X5 Y3] ====')
learning rate =  0.0005202886883246643
0.001132025959524321 0.001132025959524321 0.006718044634908438
0.0011266825655256024 0.0011266825655256024 0.007385974749922752
0.001115817801754151 0.001115817801754151 0.006051988806575537
0.001108513929011084 0.001108513929011084 0.005939379334449768
0.0011011883809449968 0.0011011883809449968 0.00602568918839097
0.0010931642716170173 0.0010931642716170173 0.005556545685976744
0.001085741570398634 0.001085741570398634 0.005114721134305
0.0010779007821861575 0.0010779007821861575 0.005106126423925161
0.0010712994428413605 0.0010712994428413605 0.005328225437551737
0.0010648792305447336 0.0010648792305447336 0.005452575162053108
0.0010573437498946595 0.0010573437498946595 0.0053697265684604645
0.0010506075680151454 0.0010506075680151454 0.005128644872456789
0.0010435465849030479 0.0010435465849030479 0.004847139585763216
iteration: 14 | epoch: 533 |   loss: 0.001044  |   KL divergence: 0.001044  |  JS divergence: 0.000292
('==== Found maximium gradient [0.0023983927, 0.0023928946, 0.002363504] of '
 'gate e^[Y1 Z9], e^[X1 Y7], e^[Y0 Z9] ====')
learning rate =  0.00047699593357233686
0.0010373992413235977 0.0010373992413235977 0.0062376586720347404
0.0010298682705673866 0.0010298682705673866 0.006506855133920908
0.001020392415519323 0.001020392415519323 0.005539880134165287
0.0010136615711145674 0.0010136615711145674 0.005799838341772556
0.001007790337944747 0.001007790337944747 0.005723165813833475
0.0010016762508858567 0.0010016762508858567 0.005396930035203695
0.0009961158829688466 0.0009961158829688466 0.00526432553306222
0.0009900904862983186 0.0009900904862983186 0.005354404915124178
0.000984866682610295 0.000984866682610295 0.005521554499864578
0.0009796616750387017 0.0009796616750387017 0.005597500130534172
0.0009736883047717869 0.0009736883047717869 0.0055099474266171455
0.0009672864504065744 0.0009672864504065744 0.005350509192794561
0.0009608880887483297 0.0009608880887483297 0.005262562073767185
0.0009553065996096883 0.0009553065996096883 0.0052467635832726955
0.0009500052659122261 0.0009500052659122261 0.005195395555347204
0.0009446436994441772 0.0009446436994441772 0.005079240538179874
0.0009393474296918059 0.0009393474296918059 0.004960736725479364
iteration: 15 | epoch: 550 |   loss: 0.000939  |   KL divergence: 0.000939  |  JS divergence: 0.000263
('==== Found maximium gradient [0.002440224, 0.002314245, 0.0022994487] of '
 'gate e^[X5 Y2], e^[X6 Y2], e^[X7 Y2] ====')
learning rate =  0.0004704308397209701
0.000933353346437955 0.000933353346437955 0.006358762737363577
0.0009266795917350775 0.0009266795917350775 0.006313818972557783
0.0009189760473053332 0.0009189760473053332 0.004857676103711128
iteration: 16 | epoch: 553 |   loss: 0.000919  |   KL divergence: 0.000919  |  JS divergence: 0.000257
('==== Found maximium gradient [0.0019900128, 0.0015549574, 0.001508683] of '
 'gate e^[X3 Y4], e^[Y5 Z1], RY[7] ====')
learning rate =  0.00033968943359980624
0.0009126680974012273 0.0009126680974012273 0.005951613653451204
0.000906516558736776 0.000906516558736776 0.005728320684283972
0.0009009394390874598 0.0009009394390874598 0.005343291908502579
0.0008949161591791897 0.0008949161591791897 0.005079546943306923
0.0008888548173627508 0.0008888548173627508 0.0049382587894797325
iteration: 17 | epoch: 558 |   loss: 0.000889  |   KL divergence: 0.000889  |  JS divergence: 0.000249
('==== Found maximium gradient [0.002055403, 0.0018656376, 0.0016826793] of '
 'gate e^[Y1 Z4], e^[X2 Y4], e^[Y5 Z0] ====')
learning rate =  0.00037481897140989275
0.000883750217520958 0.000883750217520958 0.005855407565832138
0.0008780380392612679 0.0008780380392612679 0.006772405467927456
0.0008693857538443048 0.0008693857538443048 0.00559066841378808
0.0008621865105382965 0.0008621865105382965 0.0056390115059912205
0.0008556120106467203 0.0008556120106467203 0.005743883550167084
0.0008487463651992398 0.0008487463651992398 0.005379570182412863
0.0008411556812578019 0.0008411556812578019 0.004965855740010738
iteration: 18 | epoch: 565 |   loss: 0.000841  |   KL divergence: 0.000841  |  JS divergence: 0.000235
('==== Found maximium gradient [0.0019115843, 0.0016108727, 0.0015107993] of '
 'gate e^[Y5 Z2], e^[X2 Y4], e^[Y1 Z0] ====')
learning rate =  0.0003372749484872573
0.0008351329543615175 0.0008351329543615175 0.005636021960526705
0.0008288786052769221 0.0008288786052769221 0.006531718652695417
0.0008212506018736541 0.0008212506018736541 0.005489751230925322
0.0008144900280973524 0.0008144900280973524 0.005205965600907803
0.0008082067358224142 0.0008082067358224142 0.005224263295531273
0.0008010524855432352 0.0008010524855432352 0.0051802233792841434
0.0007939996363395033 0.0007939996363395033 0.005107893142849207
0.0007877444631619378 0.0007877444631619378 0.005011510103940964
0.0007817723564967427 0.0007817723564967427 0.004999061580747366
iteration: 19 | epoch: 574 |   loss: 0.000782  |   KL divergence: 0.000782  |  JS divergence: 0.000220
('==== Found maximium gradient [0.0015813928, 0.0014775684, 0.0014743614] of '
 'gate e^[Y5 Z3], e^[X2 Y5], e^[Y5 Z1] ====')
learning rate =  0.00030238503638766595
0.0007757235954509551 0.0007757235954509551 0.005710915196686983
0.0007700253966250114 0.0007700253966250114 0.005932296626269817
0.0007628858929777135 0.0007628858929777135 0.005287111736834049
0.0007573899960084476 0.0007573899960084476 0.005138000939041376
0.0007512968974365425 0.0007512968974365425 0.005029709078371525
0.0007456595314936682 0.0007456595314936682 0.005075094290077686
0.00073938396641087 0.00073938396641087 0.005107486620545387
0.0007345564207464604 0.0007345564207464604 0.00501705426722765
0.0007284170484611035 0.0007284170484611035 0.004879683256149292
iteration: 20 | epoch: 583 |   loss: 0.000728  |   KL divergence: 0.000728  |  JS divergence: 0.000205
('==== Found maximium gradient [0.0016572936, 0.0013269635, 0.0013122607] of '
 'gate e^[Y5 Z0], e^[Y6 Z0], e^[X1 Y8] ====')
learning rate =  0.00028820090421025166
0.0007238794683465656 0.0007238794683465656 0.005449536256492138
0.0007190718959516329 0.0007190718959516329 0.006734449416399002
0.0007119671610802138 0.0007119671610802138 0.005121040157973766
0.0007071612510200161 0.0007071612510200161 0.005021951161324978
0.000702019119556471 0.000702019119556471 0.005052213557064533
0.0006963122277369406 0.0006963122277369406 0.004893511533737183
iteration: 21 | epoch: 589 |   loss: 0.000696  |   KL divergence: 0.000696  |  JS divergence: 0.000196
('==== Found maximium gradient [0.0013164933, 0.0011865266, 0.0011616684] of '
 'gate e^[Y6 Z1], e^[Y1 Z5], e^[X3 Y4] ====')
learning rate =  0.0002446895746765569
0.0006911125861050822 0.0006911125861050822 0.005264153704047203
0.0006868053280101329 0.0006868053280101329 0.006016346625983715
0.0006802231845193116 0.0006802231845193116 0.004984226077795029
iteration: 22 | epoch: 592 |   loss: 0.000680  |   KL divergence: 0.000680  |  JS divergence: 0.000192
('==== Found maximium gradient [0.0013941412, 0.0012207947, 0.0011459652] of '
 'gate e^[Y6 Z2], e^[Y5 Z4], e^[X0 Y2] ====')
learning rate =  0.0002515871280293241
0.0006750125465417918 0.0006750125465417918 0.005378595553338528
0.000670438968799528 0.000670438968799528 0.006041110027581453
0.0006642766441141288 0.0006642766441141288 0.0054834201000630856
0.0006587971461892906 0.0006587971461892906 0.005231913644820452
0.0006539168970068631 0.0006539168970068631 0.0053110504522919655
0.0006484207590768167 0.0006484207590768167 0.005284151993691921
0.0006434502481414685 0.0006434502481414685 0.00502164289355278
0.0006382024422949198 0.0006382024422949198 0.0047842105850577354
iteration: 23 | epoch: 600 |   loss: 0.000638  |   KL divergence: 0.000638  |  JS divergence: 0.000180
('==== Found maximium gradient [0.0011902764, 0.0010813148, 0.0010621302] of '
 'gate e^[Y5 Z1], e^[X4 Y5], e^[Y2 Z1] ====')
learning rate =  0.00022253449339810051
0.0006342291197854226 0.0006342291197854226 0.005163984838873148
0.0006295756038719337 0.0006295756038719337 0.005540026351809502
0.0006235225357318628 0.0006235225357318628 0.005165048874914646
0.000619297871135441 0.000619297871135441 0.005018200725317001
0.0006153940661106838 0.0006153940661106838 0.005051827058196068
0.000610363626794308 0.000610363626794308 0.004922875668853521
iteration: 24 | epoch: 606 |   loss: 0.000610  |   KL divergence: 0.000610  |  JS divergence: 0.000172
('==== Found maximium gradient [0.0010995991, 0.0009161526, 0.00091017416] of '
 'gate e^[Y4 Z1], e^[Y6 Z0], e^[X3 Y6] ====')
learning rate =  0.00019585269613556978
0.0006055870514283889 0.0006055870514283889 0.005089215002954006
0.0006013747996887909 0.0006013747996887909 0.005529206711798906
0.0005965982135271613 0.0005965982135271613 0.005120192188769579
0.000592841976301621 0.000592841976301621 0.005171727389097214
0.0005878117545097908 0.0005878117545097908 0.005169032607227564
0.0005832708749382556 0.0005832708749382556 0.005028852261602879
0.0005795536497425783 0.0005795536497425783 0.004949165973812342
iteration: 25 | epoch: 613 |   loss: 0.000580  |   KL divergence: 0.000580  |  JS divergence: 0.000163
('==== Found maximium gradient [0.0009589215, 0.0009365875, 0.00092766556] of '
 'gate e^[Y6 Z1], e^[X3 Y6], e^[Y3 Z0] ====')
Convergence criterion has reached, break the loop!
