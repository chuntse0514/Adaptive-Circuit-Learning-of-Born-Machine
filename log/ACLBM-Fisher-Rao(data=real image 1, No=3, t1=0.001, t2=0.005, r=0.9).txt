nohup: ignoring input
('==== Found maximium gradient [0.18926477, 0.18926477, 0.18926476] of gate '
 'RY[1], e^[X2 Y1], e^[X8 Y1] ====')
learning rate =  0.037852952916034216
0.059723894330686096 0.17714886732512283 0.44898778200149536
0.04838686004732518 0.15783911236167836 0.2364475429058075
0.04579868781228178 0.15255073769727795 0.3502858579158783
0.044630125913589606 0.1504621731215544 0.3102734386920929
0.043717953615275085 0.14924427687402841 0.19865241646766663
0.04312390096925588 0.1487934283461126 0.18830282986164093
0.04329119549958197 0.14953762268746246 0.2611181139945984
0.04274921930464569 0.14877714274820122 0.2697175145149231
0.04157823474171293 0.14665973830837783 0.21095165610313416
0.04058471008195616 0.14467067559822036 0.1349012553691864
0.040327715516989236 0.14391980472349739 0.1392541527748108
0.04051088427974406 0.14404116521248728 0.1893216073513031
0.04037459404621496 0.1438214236194324 0.19891469180583954
0.03980274274601979 0.1430519221597837 0.16066822409629822
0.039200089777604426 0.14232620624739656 0.10601601004600525
0.03891545096567695 0.14211398727929178 0.10232286155223846
0.038942349121543623 0.14228762202741962 0.14072471857070923
0.03899773014365003 0.14231675621341403 0.15637318789958954
0.038930105829636324 0.1419953176785197 0.1357952505350113
0.03882641827284217 0.14156577567332204 0.09648441523313522
0.03878725118460692 0.14127201003021628 0.08125493675470352
0.038763536476723114 0.14106676961470185 0.10253521800041199
0.03861779786881339 0.14074352244644459 0.11477037519216537
0.03834210442974703 0.14031720564802677 0.09899576008319855
0.038106484667644276 0.14008060302155645 0.06580612808465958
0.03805082830602972 0.14022298507418637 0.05688063055276871
0.03811083728264321 0.14053661298327488 0.08279319107532501
0.03809996351768941 0.14061651553733664 0.0978098213672638
0.03793887777704963 0.14029641476202856 0.08691665530204773
0.03775314645821781 0.1398316602609869 0.05680476129055023
0.03770544945718296 0.13957115537144843 0.0380634143948555
0.037797162463563704 0.1395797360303834 0.05627460777759552
0.037878774092722174 0.13963539272717668 0.06970807164907455
0.03784428062709618 0.13956699091221048 0.06266439706087112
0.03774266302485631 0.13945134614570523 0.043481264263391495
0.03767411234520324 0.13943161695563508 0.03959471359848976
0.037644792477842665 0.13947164413801158 0.054133061319589615
0.037584203041901534 0.13940828037792502 0.059170886874198914
0.03747952920038554 0.1392078657390744 0.04665106534957886
0.03741049765520059 0.1390257028987098 0.02548253908753395
0.03743465533216162 0.13899437769936326 0.02707880176603794
0.037497443692730587 0.13905462208665684 0.04250192269682884
0.037497163017288455 0.13904930647942146 0.04518341273069382
0.037415628442450194 0.13894425258731835 0.03401140868663788
0.037327974568051016 0.13884809012782634 0.022711068391799927
0.037291273310870336 0.13882347652998225 0.02937888167798519
0.037288008203611334 0.13881462540540762 0.036626264452934265
0.03727871475213239 0.1387489005080383 0.0324585996568203
0.037266430339498585 0.13865174777239933 0.020519662648439407
0.03727373632762357 0.1385969023183287 0.018126603215932846
0.0372855597389836 0.13858487747202058 0.02655569463968277
0.03726322145750502 0.13855971450798155 0.02836223691701889
0.037200007112745355 0.1385020982026474 0.02176625281572342
0.0371300148105576 0.13844800313115396 0.016387049108743668
0.03708208762480375 0.13841572747608344 0.02082638628780842
0.037055925773975415 0.13838070028541902 0.023744462057948112
0.03704316784460257 0.13832420679543056 0.019376147538423538
0.0370453515835569 0.1382658978615816 0.012482940219342709
0.03705955877125675 0.13822744375327534 0.014368955045938492
0.037067374083089764 0.138200021104049 0.018837517127394676
0.0370521059419229 0.13816518132977637 0.017944233492016792
0.03701640495072185 0.138121365591554 0.013763295486569405
0.036975883856327656 0.1380789633437162 0.013344954699277878
0.036941397626495426 0.13803823564130685 0.015615521930158138
0.03691589452768812 0.13799335271208377 0.014511450193822384
0.03690061486439976 0.13794940454817137 0.010576399974524975
0.03689250029048631 0.13791256320222087 0.010239389725029469
0.03688224074385996 0.1378794806868768 0.0131834726780653
0.03686155653850556 0.13784231309980058 0.013399489223957062
0.03683197438692917 0.13780050044181213 0.010764794424176216
0.0368027054150028 0.13775976457675915 0.00955328717827797
0.03678114499303275 0.13772147723763548 0.010862378403544426
0.03676859094132952 0.13768180939314598 0.01075739786028862
0.03676406176352655 0.13764602216127111 0.008921527303755283
0.03676011654018857 0.1376091977916879 0.00869976356625557
0.036750330015713484 0.13757455112707392 0.010113720782101154
0.03672908024730926 0.1375370637233666 0.009963327087461948
0.036698972070239345 0.13749881511367948 0.008217474445700645
0.03666707602953292 0.13746091497507276 0.007672244682908058
0.03664057844882066 0.13742479401061208 0.008597596548497677
0.03662278774838232 0.13739020318206396 0.008491707034409046
0.036612328120141646 0.13735478711444304 0.007412919774651527
0.036605259044976274 0.1373187583317933 0.0073410519398748875
0.03659681813581766 0.13728430820872098 0.007990684360265732
0.036582946492459384 0.1372502430362932 0.007709291763603687
0.03656291484190824 0.1372137381342715 0.006896290462464094
0.03654167299754135 0.13718141579946874 0.006921839900314808
0.036521152373327856 0.13714725630506988 0.007235293742269278
0.03650410324531135 0.13711379829083012 0.006831684615463018
0.03649026438463074 0.1370807695979526 0.006353047676384449
0.03647769393671367 0.137048534171036 0.00659223273396492
0.036463651130857005 0.1370161008462069 0.006773245986551046
0.03644745358145111 0.13698472535763884 0.0063557750545442104
0.036429537995132294 0.13695201147599292 0.0060024866834282875
0.036412720052856826 0.13692043782565977 0.006112647708505392
0.03639843352110023 0.1368896638480593 0.006128939799964428
0.036386731192395785 0.1368599811822264 0.005920710042119026
0.0363756293967278 0.13682974624602748 0.005886444356292486
0.03636336453553215 0.1368000046550225 0.005918395705521107
0.03634899734531252 0.1367716956213624 0.005700547248125076
0.03633235742862281 0.136742671758639 0.005468783900141716
0.036315698119119846 0.13671501350763077 0.005492704920470715
0.03630068525404668 0.13668789732682873 0.005513145588338375
0.036287552300275416 0.13665846126901735 0.005369848571717739
0.03627745232246728 0.13663244372977307 0.005269302520900965
0.03626767186902115 0.1366055208953636 0.005250195972621441
0.03625711063970708 0.13657942546164276 0.005155169405043125
0.03624444579257345 0.136552042338265 0.005042754113674164
0.036231227877079344 0.1365277191046412 0.005019813310354948
0.0362175822540788 0.1365029924966774 0.004978246986865997
iteration: 1 | epoch: 111 |   loss: 0.136503  |   KL divergence: 0.036218  |  JS divergence: 0.009225
('==== Found maximium gradient [0.06485418, 0.062413514, 0.055410616] of gate '
 'e^[Y9 Z8], e^[Y8 Z9], e^[Y8 Z1] ====')
learning rate =  0.012204834670251
0.03620440907882235 0.13647718793084374 0.1058088093996048
0.03536873336291648 0.13501358234489985 0.0630209743976593
0.035491351437163865 0.1348429100342461 0.06698452681303024
0.0355884934096038 0.13501895845668052 0.07229942083358765
0.03551523685926129 0.1350144050623826 0.06595338135957718
0.0354396134725376 0.1349810495874661 0.06991461664438248
0.03528581257501482 0.13469970041132465 0.049033720046281815
0.03521013946450006 0.1345089488994483 0.02049783244729042
0.03528752744786662 0.13461496244739524 0.04099852219223976
0.03534306104588612 0.1347518551767116 0.0542718842625618
0.03528285712259346 0.13474093818931343 0.0489954948425293
0.03518731070403215 0.13467790181280742 0.04262569919228554
0.03512115043564546 0.13462236212742315 0.04251214861869812
0.03507886433547114 0.13452924213863704 0.035024918615818024
0.03507959312060298 0.13444556938686666 0.02045808918774128
0.035146787033575 0.13445623645148114 0.02448238618671894
0.035230675974310344 0.1345191083912772 0.03642386198043823
0.03525838140723197 0.13452981409152223 0.03633258119225502
0.03523229603397905 0.13449053948518777 0.027528194710612297
0.03520154030185797 0.1344676271687345 0.023664573207497597
0.03518430002949781 0.13446160807787946 0.025599930435419083
0.035174683471864754 0.13444602405303477 0.022758452221751213
0.03517678882399797 0.13443480116072856 0.019433172419667244
0.03518763763445373 0.1344446905946773 0.02378063090145588
0.03517911240645077 0.13444507027861224 0.026225797832012177
0.03513758118729003 0.1344141389742915 0.0202325452119112
0.035090512889702316 0.13438613839137611 0.011506269685924053
0.0350675065243409 0.13438823766316993 0.01397575531154871
0.03506902837368288 0.13439931379229694 0.017779605463147163
0.035085663947669134 0.13440085188715242 0.016618113964796066
0.03511199604328918 0.1344008740041523 0.016276013106107712
0.03513432025971636 0.1343998585747685 0.017750700935721397
0.035136134334049086 0.134385259214504 0.014848784543573856
0.03512242906324439 0.13436828159161382 0.007699738722294569
0.03511109705521966 0.13436747845349545 0.008649813942611217
0.03510791022508941 0.13437384698101776 0.013494259677827358
0.03510919390892693 0.13437202079923707 0.013178450986742973
0.03511334381796939 0.13436445066514585 0.010741380974650383
0.03511721643025719 0.13436083748336866 0.0106203006580472
0.035112711141357306 0.13435756876645716 0.009385887533426285
0.0350997424371543 0.13435213294094692 0.005858795251697302
0.0350893525592896 0.13435425082088925 0.007676658686250448
0.03508684744132562 0.13435930734589716 0.010829737409949303
0.03508932241099378 0.13435325276750987 0.009744349867105484
0.03509656713614907 0.13434453679220437 0.006561224348843098
0.0351048607129277 0.13434118825304214 0.006159925367683172
0.03510634967732596 0.13433932063826926 0.006054533179849386
0.0350998600072521 0.1343379629454409 0.005176994483917952
0.03509158254995118 0.13434010367802363 0.006993430200964212
0.03508493542028277 0.1343386095267343 0.007994502782821655
0.0350820299500554 0.13433349424531213 0.005736391060054302
0.03508307633780142 0.1343296983620486 0.0032801891211420298
iteration: 2 | epoch: 163 |   loss: 0.134330  |   KL divergence: 0.035083  |  JS divergence: 0.008930
('==== Found maximium gradient [0.054079913, 0.05363596, 0.0108104395] of gate '
 'e^[Y1 Z2], e^[Y2 Z1], e^[X1 Y2] ====')
learning rate =  0.00888318108671053
0.03508386835035181 0.13432721037545386 0.07707684487104416
0.03511834844100065 0.13463284839242293 0.11180159449577332
0.034687571723360074 0.13361143201689418 0.03117893636226654
0.03499880532115008 0.13396292811807617 0.06753376871347427
0.03528736109496721 0.1344158890513408 0.0963255912065506
0.03512279715206228 0.13415770181325956 0.0797533318400383
0.03481444738945509 0.1337005714107069 0.04062655195593834
0.0346637263452638 0.13355054705460176 0.0238131545484066
0.03470032600593444 0.13370800146054157 0.051707543432712555
0.034765840466938334 0.13385021449496523 0.0635061115026474
0.03476874267592171 0.13381556606492181 0.0563979409635067
0.034741743266106004 0.13369211403186373 0.040136050432920456
0.03473867077546291 0.13361166138496916 0.03169068694114685
0.034764201683374754 0.13360804993598732 0.038458213210105896
0.03478040141851156 0.13362247082203488 0.04374660551548004
0.03476113675104042 0.13360463835044975 0.03981521353125572
0.03471719737402871 0.1335609785829842 0.02960941568017006
0.034680244328589606 0.13353549164946266 0.023994896560907364
0.03466903517914648 0.13354899435910725 0.030301962047815323
0.03467179146116689 0.13357036376403592 0.036610182374715805
0.03466476981629207 0.13355242311755894 0.03484819084405899
0.03464709884443689 0.1334999782163435 0.024810105562210083
0.03463815724626033 0.1334559752909271 0.012937851250171661
0.03465127907830376 0.13345636554755758 0.0162979643791914
0.03467206667375706 0.13348099092776472 0.025494292378425598
0.03467736179901741 0.13349137169138697 0.028333833441138268
0.03465995708637413 0.13347318600543673 0.023997172713279724
0.03463110696570988 0.1334405068184482 0.015967389568686485
0.03460853303740216 0.1334210910177992 0.012087371200323105
0.0345989498532666 0.13341717083399626 0.015995727851986885
0.0346006701657328 0.13342076253717092 0.019196098670363426
0.03460757191880767 0.13341937410431082 0.018405040726065636
0.034615606646770686 0.13341048661556965 0.015109241008758545
0.034622781830385 0.13339937883687514 0.013282646425068378
0.03462633376865269 0.13339135036924585 0.01434115506708622
0.034621599532613484 0.1333839035332546 0.01458023302257061
0.034607480052517084 0.1333743232461737 0.011928416788578033
0.03459007820436409 0.13336782138063344 0.008664223365485668
0.03457632774307144 0.13336611532888706 0.009871655143797398
0.03456898882512473 0.13336476806940628 0.012852049432694912
0.03456760301788274 0.13335968498274967 0.012957880273461342
0.03456999214059125 0.13334652237972175 0.009484730660915375
0.034577122572158794 0.13333739310255843 0.004987907595932484
iteration: 3 | epoch: 206 |   loss: 0.133337  |   KL divergence: 0.034577  |  JS divergence: 0.008801
('==== Found maximium gradient [0.004237305, 0.0036984389, 0.0032297338] of '
 'gate e^[Y8 Z1], e^[Y1 Z8], e^[Y9 Z8] ====')
learning rate =  0.0007489048435407793
0.034585178070379384 0.13333352186033254 0.009055498987436295
0.034556826799723224 0.13332874561460406 0.006689948961138725
0.03455579880644657 0.13332861098632778 0.00784251093864441
0.03456037782383393 0.13332513088760917 0.006488848477602005
0.034565489098376374 0.13332361503978454 0.005715563893318176
0.0345672083639762 0.1333217652194237 0.00546288164332509
0.034565710221128566 0.13331995167989993 0.005222488660365343
0.034562141799547925 0.1333182569739695 0.004832100123167038
iteration: 4 | epoch: 214 |   loss: 0.133318  |   KL divergence: 0.034562  |  JS divergence: 0.008798
('==== Found maximium gradient [0.0022161491, 0.0021474997, 0.0018595821] of '
 'gate e^[Y8 Z1], e^[X8 Y1], e^[Y1 Z8] ====')
learning rate =  0.00041603072217981634
0.03455823080289636 0.1333157655562791 0.005697960499674082
0.03456143400883704 0.13331518433414766 0.006350385025143623
0.03455664876524733 0.13331409629424845 0.0063787419348955154
0.034552921045479065 0.13331165602011777 0.0047394209541380405
iteration: 5 | epoch: 218 |   loss: 0.133312  |   KL divergence: 0.034553  |  JS divergence: 0.008797
('==== Found maximium gradient [0.0025646687, 0.0018403463, 0.0017055355] of '
 'gate RY[2], e^[X8 Y9], e^[Y9 Z8] ====')
learning rate =  0.0004142986044136242
0.03455128371310234 0.13330930049249512 0.005882638040930033
0.03455726565972899 0.13330844275235498 0.009021734818816185
0.03455632314099286 0.13330427603454723 0.004865854978561401
iteration: 6 | epoch: 221 |   loss: 0.133304  |   KL divergence: 0.034556  |  JS divergence: 0.008797
('==== Found maximium gradient [0.0021949795, 0.0019959675, 0.0018662214] of '
 'gate e^[Y8 Z1], e^[X1 Y2], RY[9] ====')
learning rate =  0.00040471559236634005
0.034553494347178 0.1333026241155477 0.006315378937870264
0.03455391404629987 0.13330001885051054 0.007531206123530865
0.03454551691769188 0.1332966818303256 0.0061540850438177586
0.03454049418512774 0.13329269004566843 0.00465089175850153
iteration: 7 | epoch: 225 |   loss: 0.133293  |   KL divergence: 0.034540  |  JS divergence: 0.008795
('==== Found maximium gradient [0.0022692042, 0.0020992756, 0.0014040661] of '
 'gate e^[X8 Y1], RY[2], e^[X2 Y1] ====')
learning rate =  0.00039204844444952144
0.034539167575698276 0.13329049498965942 0.006719566415995359
0.03454685196588009 0.13328821109019498 0.009965642355382442
0.034543896312713346 0.1332841780049568 0.0061871823854744434
0.03453848411855172 0.13327974720351585 0.004894275218248367
iteration: 8 | epoch: 229 |   loss: 0.133280  |   KL divergence: 0.034538  |  JS divergence: 0.008793
('==== Found maximium gradient [0.0023684439, 0.0017762334, 0.0015699258] of '
 'gate e^[X8 Y1], e^[Y8 Z1], e^[X1 Y2] ====')
learning rate =  0.00038694011253276485
0.0345326242785266 0.13327626602101655 0.007389056496322155
0.034538705192128205 0.13327507726905544 0.011848642490804195
0.034532444356064645 0.13326756393009717 0.007063884753733873
0.03452669283265737 0.13326456055935962 0.005507835187017918
0.03452160881725563 0.1332614370046855 0.00817019958049059
0.034518086803066464 0.13325650260695648 0.0071264347061514854
0.03451698832813581 0.13325286893260174 0.004907979629933834
iteration: 9 | epoch: 236 |   loss: 0.133253  |   KL divergence: 0.034517  |  JS divergence: 0.008790
('==== Found maximium gradient [0.0013878454, 0.0013281247, 0.0010986328] of '
 'gate e^[X1 Y8], e^[X8 Y9], RY[2] ====')
learning rate =  0.0002555262327330847
0.03451756107957447 0.13324935179612069 0.00590709550306201
0.034520125048528076 0.13324470184272866 0.007686270866543055
0.03451936382338528 0.13324171467482784 0.005101941525936127
0.03451713327123536 0.1332384695011554 0.004659684374928474
iteration: 10 | epoch: 240 |   loss: 0.133238  |   KL divergence: 0.034517  |  JS divergence: 0.008788
('==== Found maximium gradient [0.0016390085, 0.0010779737, 0.0010179281] of '
 'gate RY[9], e^[X1 Y2], RY[8] ====')
learning rate =  0.0002552006826681923
0.03451268623635734 0.13323324002049458 0.006354762241244316
0.03451258649655734 0.13323253412477623 0.007483082357794046
0.034511456194472406 0.1332281339486501 0.00582017470151186
0.03450886022494625 0.13322580262451003 0.004986403975635767
iteration: 11 | epoch: 244 |   loss: 0.133226  |   KL divergence: 0.034509  |  JS divergence: 0.008786
('==== Found maximium gradient [0.001636982, 0.00086608727, 0.0008188486] of '
 'gate RY[2], e^[Y8 Z9], RY[1] ====')
learning rate =  0.0002338184500923892
0.03450372070066592 0.1332211960817935 0.005973948165774345
0.034506658389667724 0.13321956023857073 0.007348576094955206
0.034502303259510765 0.13321339170302302 0.005573451053351164
0.034499050909846674 0.13320951617521964 0.0045598773285746574
iteration: 12 | epoch: 248 |   loss: 0.133210  |   KL divergence: 0.034499  |  JS divergence: 0.008784
('==== Found maximium gradient [0.0012267828, 0.00087828643, 0.0008342321] of '
 'gate RY[2], e^[X8 Y9], e^[Y8 Z9] ====')
learning rate =  0.00019907539712480497
0.03449498175863069 0.1332078913507244 0.005723316688090563
0.03449640044796552 0.13320387284264792 0.006145603954792023
0.03449577813331795 0.13319852451176034 0.00526789203286171
0.03449289097147916 0.13319660160228997 0.004939962178468704
iteration: 13 | epoch: 252 |   loss: 0.133197  |   KL divergence: 0.034493  |  JS divergence: 0.008782
('==== Found maximium gradient [0.0015250014, 0.0012469292, 0.00074022816] of '
 'gate e^[X1 Y2], RY[9], e^[Y8 Z9] ====')
learning rate =  0.00024299243425237685
0.0344879719496826 0.13319283334807303 0.005767228547483683
0.03448762900523557 0.13318741103004425 0.007960192859172821
0.03448912619545028 0.1331848496095425 0.007787755690515041
0.034482840933640506 0.133181174623573 0.006915452890098095
0.034476275663936115 0.13317604320765472 0.0060348776169121265
0.0344738523003583 0.13317421721670855 0.006335925776511431
0.0344740603850728 0.13317104895719514 0.005880928598344326
0.03447553225155789 0.13316846807477245 0.004981367848813534
iteration: 14 | epoch: 260 |   loss: 0.133168  |   KL divergence: 0.034476  |  JS divergence: 0.008778
('==== Found maximium gradient [0.0020119385, 0.0016169543, 0.0010183519] of '
 'gate e^[Y8 Z1], e^[Y1 Z8], e^[X1 Y2] ====')
learning rate =  0.0003204055458100949
0.03447475610652866 0.13316085856035445 0.0056379507295787334
0.03445828900442682 0.13316040380879463 0.012514005415141582
0.03446492784720276 0.13315067432268368 0.005618931259959936
0.03447090987214654 0.13314826970145768 0.008743773214519024
0.034465669713930586 0.1331412501676877 0.008334899321198463
0.03445569505705098 0.13313389552577257 0.005832192488014698
0.03444783641887849 0.13313032745289147 0.005850657355040312
0.03444359876701114 0.13312546495710345 0.00692738639190793
0.0344423754332747 0.13311842505278504 0.006799260154366493
0.03444375107347537 0.1331150771457309 0.00593778258189559
0.03444373631478596 0.13310948002858153 0.005302132107317448
0.03444138451464224 0.1331032819331868 0.005142961163073778
0.03443753697800784 0.1330998104393165 0.005311271175742149
0.0344318227628219 0.13309391621616015 0.005421164445579052
0.034426280164341204 0.13308769650928268 0.005100701004266739
0.03442285541624275 0.13308455570609962 0.004530846141278744
iteration: 15 | epoch: 276 |   loss: 0.133085  |   KL divergence: 0.034423  |  JS divergence: 0.008767
('==== Found maximium gradient [0.0007443428, 0.00068164826, 0.0006174966] of '
 'gate RY[2], e^[Y8 Z1], e^[X8 Y1] ====')
Convergence criterion has reached, break the loop!
